---
pdf_document: default
authors: "Alvaro Cervan, Luca Renz, Rafaella Miranda-Sousa"
date: "`r Sys.Date()`"
output:
  #word_document: default
  #pdf_document: default
  html_document: default
    
title: "ML1"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, cache=TRUE)
```


```{r load_packages, echo =FALSE, cache=TRUE}

#Install and import libraries.
if (!require("car")) install.packages("car")  # Support Vector Machine (SVM)
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("ROI")) install.packages("ROI")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("lubridate")) install.packages("lubridate")
if (!require("corrplot")) install.packages("corrplot")
if (!require("dplyr")) install.packages("dplyr")
if (!require("GGally")) install.packages("GGally")
if (!require("mgcv")) install.packages("mgcv")  # Generalised Additive Model (GAM)
if (!require("nnet")) install.packages("nnet")  # Neural Networks
if (!require("e1071")) install.packages("e1071")  # Support Vector Machine (SVM)
if (!require("MASS")) install.packages("MASS")
if (!require("arm")) install.packages("arm")
if (!require("caret")) install.packages("caret")
if (!require("pscl")) install.packages("pscl")
if (!require("doParallel")) install.packages("doParallel")
if (!require("rcompanion")) install.packages("rcompanion")
if (!require("gam")) install.packages("gam")
if (!require("MLmetrics")) install.packages("MLmetrics")
if (!require("mccr")) install.packages("mccr")
if (!require("NeuralNetTools")) install.packages("NeuralNetTools")

library(car)
library(readr,quietly = T)
library(ggplot2, quietly = T)
library(ROI, quietly = T)
library(tidyverse, quietly = T)
library(lubridate, quietly = T)
library(corrplot, quietly = T)
library(dplyr)
library(GGally)
library(mgcv)  # For Generalised Additive Models
library(nnet)  # For Neural Networks
library(e1071)  # For Support Vector Machine
library(arm)
library(caret)
library(pscl) #zeroinflated
library(rcompanion)
library(gam)
library(doParallel)#SVM Parallelisierung
library(MLmetrics)  # For MCC calculation
library(mccr)
library(NeuralNetTools) # For Neural Network visualization

```
# Introduction
Once of the major challenges for insurances is to estimate the appropriate premiums to charge each customer while not risking to lose any money. Therefore, this project aims at supporting an Ethiopian Insurance company to understand how their customers can benefit from having the most accurate and fair premium as they need and have to pay. Machine Learning helps in this case enormely to understand, what factors have a larger impact on the premium and how customers can be classified accordingly.

In this document, the reader may find different algorithms to solve various aspects of the premium-calculations.

# Data Preprocessing
In order to apply such algorithms, the data had to be pre-processed. This process can be found below.

```{r data_prep, echo=FALSE, cache=TRUE}

# DATA LOAD
raw_dat_motor <- read_csv("data/motor_data14-2018.csv", show_col_types = FALSE)

dim(raw_dat_motor)
str(raw_dat_motor)

#DATA PREP

#EFFECTIVE_YR
#Entfernen der Spalte EFFECTIVE_YR (Hat keinen Nutzen, da nicht entziffert werden kann)
raw_dat_motor <- raw_dat_motor[ , !(names(raw_dat_motor) %in% "EFFECTIVE_YR")]

#CARRYING_CAPACITY
#Entfernen der Spalte CARRYING_CAPACITY
raw_dat_motor <- raw_dat_motor[ , !(names(raw_dat_motor) %in% "CARRYING_CAPACITY")]

#CLAIM_PAID
raw_dat_motor$CLAIM_PAID_USD <- ifelse(is.na(raw_dat_motor$CLAIM_PAID), 0, raw_dat_motor$CLAIM_PAID)
raw_dat_motor$CLAIM_PAID <- ifelse(raw_dat_motor$CLAIM_PAID_USD == 0, "NO", "YES")

# Entfernen von Duplikaten und Zählen der entfernten Zeilen
removed_count <- nrow(raw_dat_motor) - nrow(raw_dat_motor <- distinct(raw_dat_motor))
# Ausgabe der Anzahl der entfernten Duplikate
cat("Anzahl der entfernten Duplikate:", removed_count, "\n")

#SEX
raw_dat_motor$SEX <- factor(raw_dat_motor$SEX, 
                            levels = c(0, 1, 2), 
                            labels = c("Legal entity", "Male", "Female"))
table(raw_dat_motor$SEX)

#INSR_BEGIN
raw_dat_motor$INSR_BEGIN <- dmy(raw_dat_motor$INSR_BEGIN)

#INSR_END
raw_dat_motor$INSR_END <- dmy(raw_dat_motor$INSR_END)

#DURATION
raw_dat_motor$DURATION <- as.numeric(as.Date(raw_dat_motor$INSR_END) - as.Date(raw_dat_motor$INSR_BEGIN))
hist(raw_dat_motor$DURATION)
#Gleiche DURATION (Vertragsdauer) zur Vergleichbarkeit
raw_dat_motor <- raw_dat_motor[raw_dat_motor$DURATION == 364, ]

#Jahr des Versicherungsbeginns extrahieren und als neue Variable hinzufügen
raw_dat_motor$START_INS_YR <- year(as.Date(raw_dat_motor$INSR_BEGIN))

#INSR_TYPE
raw_dat_motor$INSR_TYPE <- factor(raw_dat_motor$INSR_TYPE, 
                                  levels = c(1201, 1202, 1204), 
                                  labels = c("Private", "Commercial", "Motor trade road risk"))

#INSURED_VALUE
#Überprüfen der Anzahl der fehlenden Werte
missing_values <- sum(is.na(raw_dat_motor$INSURED_VALUE))
cat("Fehlende Werte in INSURED_VALUE:", missing_values, "\n")

#Zusammenfassung der statistischen Kennzahlen
summary_stats <- summary(raw_dat_motor$INSURED_VALUE)
cat("Zusammenfassung der statistischen Kennzahlen von INSURED_VALUE:\n")
print(summary_stats)

#Ermittlung der Anzahl der Einträge, die 0 als Wert haben
zero_values <- sum(raw_dat_motor$INSURED_VALUE == 0, na.rm = TRUE)
cat("Anzahl der Einträge mit dem Wert 0 in INSURED_VALUE:", zero_values, "\n")

#Überprüfe, wie viele Datensätze betroffen sind
zero_insured_value <- raw_dat_motor[raw_dat_motor$INSURED_VALUE == 0, ]
cat("Anzahl der Datensätze mit INSURED_VALUE = 0:", nrow(zero_insured_value), "\n")

#Zusammenfassung der betroffenen Datensätze nach verschiedenen Variablen, um Muster zu erkennen
cat("Verteilung der Versicherungstypen (INSR_TYPE) bei INSURED_VALUE = 0:\n")
table(raw_dat_motor$INSR_TYPE) #Alle
print(table(zero_insured_value$INSR_TYPE)) #nur 0

cat("\nVerteilung der Fahrzeugtypen (TYPE_VEHICLE) bei INSURED_VALUE = 0:\n")
print(table(zero_insured_value$TYPE_VEHICLE))

cat("\nVerteilung der Fahrzeugnutzung (USAGE) bei INSURED_VALUE = 0:\n")
print(table(zero_insured_value$USAGE))

#Statistische Kennzahlen für andere Variablen bei INSURED_VALUE = 0 (z.B. PREMIUM)
cat("\nZusammenfassung der Prämien (PREMIUM) bei INSURED_VALUE = 0:\n")
summary(zero_insured_value$PREMIUM)

#Visualisierung der Fahrzeugnutzung bei INSURED_VALUE = 0
ggplot(zero_insured_value, aes(x = USAGE)) +
  geom_bar(fill = "blue", color = "black") +
  labs(title = "Verteilung der Fahrzeugnutzung bei INSURED_VALUE = 0", x = "Fahrzeugnutzung", y = "Anzahl") +
  theme_minimal()

#Fazit: Es wurde kein Zusammenhang festgestellt. Vermutlich ist der Versicherungswert von 0 darauf zurückzuführen, dass gesetzlich nur eine Haftpflichtversicherung erforderlich ist. In diesen Fällen gibt es keinen festgelegten Wert für Schäden am Fahrzeug selbst. Daher werden Datensätze mit einem INSURED_VALUE von 0 aus der Analyse entfernt, da sie keine relevanten Informationen für die Bewertung von Fahrzeugwerten enthalten.

#Entfernen der Zeilen, bei denen INSURED_VALUE gleich 0 ist
raw_dat_motor <- raw_dat_motor[raw_dat_motor$INSURED_VALUE != 0, ]

#Überprüfen, ob die Zeilen erfolgreich entfernt wurden
cat("Anzahl der verbleibenden Datensätze:", nrow(raw_dat_motor), "\n")

#Verteilung von INSURED_VALUE visualisieren (Histogramm)
ggplot(raw_dat_motor, aes(x = INSURED_VALUE)) +
  geom_histogram(binwidth = 50000, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Verteilung von INSURED_VALUE", x = "Versicherter Wert", y = "Häufigkeit") +
  theme_minimal()

#Boxplot zur Identifizierung von Ausreissern
ggplot(raw_dat_motor, aes(y = INSURED_VALUE)) +
  geom_boxplot(fill = "orange", color = "black", alpha = 0.7) +
  labs(title = "Boxplot von INSURED_VALUE", y = "Versicherter Wert") +
  theme_minimal()

#Überprüfung der statistischen Kennzahlen ohne die 0-Werte
cat("Zusammenfassung der statistischen Kennzahlen ohne 0-Werte:\n")
print(summary(raw_dat_motor$INSURED_VALUE))

#Verteilung der log-transformierten INSURED_VALUE (nur für nicht-null Werte)
ggplot(raw_dat_motor[raw_dat_motor$INSURED_VALUE > 0, ], aes(x = log(INSURED_VALUE))) +
  geom_histogram(binwidth = 0.2, fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Log-transformierte Verteilung von INSURED_VALUE (ohne Nullwerte)", x = "log(Versicherter Wert)", y = "Häufigkeit") +
  theme_minimal()

summary(raw_dat_motor$INSURED_VALUE)

#Ausreisser INSURED_VALUE entfernen 
raw_dat_motor <- raw_dat_motor[raw_dat_motor$INSURED_VALUE >= 10, ]
summary(raw_dat_motor$INSURED_VALUE)

#Workspace
rm(zero_insured_value)

#PREMIUM
#Uberpruefung ob entfernen von PREMIUM = 0 vom Datensatz valide ist
data.frame(
  PREMIUM_0_Percent = round(100 * sum(raw_dat_motor$PREMIUM == 0, na.rm = TRUE) / sum(!is.na(raw_dat_motor$PREMIUM)), 4),
  PREMIUM_NA_Percent = round(100 * sum(is.na(raw_dat_motor$PREMIUM)) / sum(!is.na(raw_dat_motor$PREMIUM)), 4),
  PREMIUM_MORE_Percent = round(100 * sum(raw_dat_motor$PREMIUM > 0, na.rm = TRUE) / sum(!is.na(raw_dat_motor$PREMIUM)), 4)
)

#Entfernen der Zeilen, bei denen PREMIUM NA oder 0 ist
raw_dat_motor <- raw_dat_motor[!(is.na(raw_dat_motor$PREMIUM) | raw_dat_motor$PREMIUM == 0), ]


# Entfernen von Duplikaten und Zählen der entfernten Zeilen
removed_count <- nrow(raw_dat_motor) - nrow(raw_dat_motor <- distinct(raw_dat_motor))
# Ausgabe der Anzahl der entfernten Duplikate
cat("Anzahl der entfernten Duplikate:", removed_count, "\n")

# OBJECT_ID
# Anzahl der Gesamtzeilen im Datensatz
total_rows <- nrow(raw_dat_motor)

# Anzahl der einzigartigen OBJECT_IDs
unique_object_ids <- length(unique(raw_dat_motor$OBJECT_ID))

# Überprüfen, ob OBJECT_IDs einmalig sind
if (total_rows == unique_object_ids) {
  cat("Die OBJECT_IDs sind einmalig.\n")
} else {
  cat("Die OBJECT_IDs sind NICHT einmalig.\n")
  cat("Anzahl der Duplikate:", total_rows - unique_object_ids, "\n")
  
  # Häufigkeit der OBJECT_IDs
  object_id_counts <- table(raw_dat_motor$OBJECT_ID)
  
  # Durchschnittliche und maximale Häufigkeit von OBJECT_ID
  avg_object_id_freq <- mean(object_id_counts)
  max_object_id_freq <- max(object_id_counts)
  
  cat("Durchschnittliche Häufigkeit der OBJECT_ID:", round(avg_object_id_freq, 3), "\n")
  cat("Maximale Häufigkeit der OBJECT_ID:", max_object_id_freq, "\n")
  
  # Häufigkeit der Kombination von OBJECT_ID, INSR_BEGIN, INSR_END, INSURED_VALUE und PREMIUM
  combo_counts <- raw_dat_motor %>%
    group_by(OBJECT_ID, INSR_BEGIN, INSR_END, INSURED_VALUE, PREMIUM) %>%
    summarise(count = n(), .groups = 'drop')
  
  # Durchschnittliche und maximale Häufigkeit der Kombination
  avg_combo_freq <- mean(combo_counts$count) # Durchschnittliche Häufigkeit der Kombination
  max_combo_freq <- max(combo_counts$count)   # Maximale Häufigkeit der Kombination
  
  cat("Durchschnittliche Häufigkeit der Kombination (OBJECT_ID, INSR_BEGIN, INSR_END, INSURED_VALUE, PREMIUM):", round(avg_combo_freq, 3), "\n")
  cat("Maximale Häufigkeit der Kombination (OBJECT_ID, INSR_BEGIN, INSR_END, INSURED_VALUE, PREMIUM):", max_combo_freq, "\n")
}

#Teilweise gibt es bei CLAIM_PAID== YES eine vervielfachung
#Korrektur bzw. entfernen dieser mehrfachen Zeilen
raw_dat_motor <- raw_dat_motor %>%
  group_by(SEX, INSR_BEGIN, INSR_END, INSR_TYPE, INSURED_VALUE, PREMIUM, OBJECT_ID, PROD_YEAR, SEATS_NUM, TYPE_VEHICLE, CCM_TON, MAKE, USAGE) %>%
  # Zähle die Anzahl der Zeilen in jeder Gruppe
  mutate(group_size = n()) %>%
  ungroup() %>%
  # Entferne Zeilen nur, wenn es eine Doppelzeile gibt und CLAIM_PAID == "NO" und CLAIM_PAID_USD <= 1
  filter(!(group_size > 1 & CLAIM_PAID == "NO" & CLAIM_PAID_USD <= 1)) %>%
  dplyr::select(-group_size)  # Entferne die Hilfsspalte

#Korrektur Wiederspruch bei INSR_TYPE
raw_dat_motor <- raw_dat_motor %>%
  group_by(SEX, INSR_BEGIN, INSR_END, INSURED_VALUE, PREMIUM, OBJECT_ID, PROD_YEAR, SEATS_NUM, TYPE_VEHICLE, CCM_TON, MAKE, USAGE) %>%
  filter(!(n() > 1 & INSR_TYPE != "Commercial")) %>%  # Behalte nur die Zeilen mit "Commercial"
  ungroup()  # Ungroup, um das Gruppierungsobjekt zu entfernen

# Clear Workspace
rm(list = setdiff(ls(), "raw_dat_motor"))


#PROD_YEAR
#Zeilen mit PROD_YEAR NA entfernen
raw_dat_motor <- raw_dat_motor[!is.na(raw_dat_motor$PROD_YEAR),]
summary(raw_dat_motor$PROD_YEAR)

#SEATS_NUM
#Analyse SEATS_NUM: Anzahl der Zeilen mit SEATS_NUM == 0, NA und anderen Werten
data.frame(
  SEATS_NUM_0 = sum(raw_dat_motor$SEATS_NUM == 0, na.rm = TRUE),
  SEATS_NUM_NA = sum(is.na(raw_dat_motor$SEATS_NUM)),
  SEATS_NUM_OTHER = sum(raw_dat_motor$SEATS_NUM > 0, na.rm = TRUE)
)
#Relativ: Prozentsatz der Zeilen mit SEATS_NUM == 0, NA oder anderen Werten
data.frame(
  SEATS_NUM_0_or_NA_Percent = 100 * sum(is.na(raw_dat_motor$SEATS_NUM) | raw_dat_motor$SEATS_NUM == 0) / nrow(raw_dat_motor),
  SEATS_NUM_OTHER_Percent = 100 * sum(raw_dat_motor$SEATS_NUM > 0, na.rm = TRUE) / nrow(raw_dat_motor)
)
#Zeilen mit SEATS_NUM NA entfernen
raw_dat_motor <- raw_dat_motor[!is.na(raw_dat_motor$SEATS_NUM),]
summary(raw_dat_motor$SEATS_NUM)

#Problematik: Es gibt SEATS_NUM mit 0

##Erstellen separater Datensätze für SEATS_NUM == 0 und SEATS_NUM > 0
#data_seats_num_0 <- subset(raw_dat_motor, SEATS_NUM == 0)
#data_seats_num_other <- subset(raw_dat_motor, SEATS_NUM > 0 & !is.na(SEATS_NUM))
## Tabellen der Fahrzeugtypen für beide Datensätze
#table(data_seats_num_0$TYPE_VEHICLE)
#table(data_seats_num_other$TYPE_VEHICLE)

##SEATS_NUM Alternative 1: Entfernen der Zeilen wo SEATS_NUM 0 oder NULL ist
##Liste der Fahrzeugtypen, bei denen SEATS_NUM == 0 unplausibel ist (Trailers and semitrailers, Tractor, Tanker werden gelassen)
#unplausible_types <- c("Automobile", "Bus", "Motor-cycle", "Pick-up", "Station Wagones", "Tanker", "Truck")
##Entfernen der Zeilen, bei denen SEATS_NUM == 0 und der Fahrzeugtyp unplausibel ist
#raw_dat_motor <- raw_dat_motor[!(raw_dat_motor$SEATS_NUM == 0 & raw_dat_motor$TYPE_VEHICLE %in% unplausible_types), ]
#
#
##SEATS_NUM Alternative 2: Entfernen der Zeilen wo SEATS_NUM 0 oder NULL ist
#raw_dat_motor <- raw_dat_motor[!(is.na(raw_dat_motor$SEATS_NUM) | raw_dat_motor$SEATS_NUM == 0), ]
#
##SEATS_NUM Alternative 3: Entfernen der SEATS_NUM-Spalte, da viele NA und schlechte Datenqualität
##raw_dat_motor <- subset(raw_dat_motor, select = -SEATS_NUM)

#TYPE_VEHICLE
#Entfernen der Zeilen, bei denen TYPE_VEHICLE == "Trade plates" Da zu wenige Auspraegungen (5)
#table(raw_dat_motor$TYPE_VEHICLE)
raw_dat_motor <- raw_dat_motor[raw_dat_motor$TYPE_VEHICLE != "Trade plates", ]

#CCM_TON
summary(raw_dat_motor$CCM_TON)
#Relativ CCM_TON 0
data.frame(
  CCM_TON_0_Percent = 100 * mean(raw_dat_motor$CCM_TON == 0, na.rm = TRUE),
  CCM_TON_MORE_Percent = 100 * mean(raw_dat_motor$CCM_TON > 0, na.rm = TRUE))
# Erstellen separater Datensätze für CCM_TON == 0 und CCM_TON > 0
data_CCM_TON_0 <- subset(raw_dat_motor, CCM_TON == 0)
data_CCM_TON_other <- subset(raw_dat_motor, CCM_TON > 0 & !is.na(CCM_TON))
# Tabellen der Fahrzeugtypen für beide Datensätze
table(data_CCM_TON_0$TYPE_VEHICLE)
table(data_CCM_TON_other$TYPE_VEHICLE)
# Liste der Fahrzeugtypen, bei denen CCM_TON == 0 unplausibel ist (Tractor,Trailers and semitrailers werden gelassen)
unplausible_types_ccm <- c("Automobile", "Bus", "Motor-cycle", "Pick-up", "Truck", "Station Wagones", "Tanker", "Special construction")
# Entfernen der Zeilen, bei denen CCM_TON == 0 und der Fahrzeugtyp unplausibel ist
raw_dat_motor <- raw_dat_motor[!(raw_dat_motor$CCM_TON == 0 & raw_dat_motor$TYPE_VEHICLE %in% unplausible_types_ccm), ]


#USAGE
#table(raw_dat_motor$USAGE)
#Wenige Ausprägungen entfernen
raw_dat_motor <- subset(raw_dat_motor, !(USAGE %in% c("Fire fighting", "Learnes", "Others")))



#MAKE
#Zeilen mit MAKE NA entfernen
raw_dat_motor <- raw_dat_motor[!is.na(raw_dat_motor$MAKE),]
#MAKE in Grossbuchstaben umwandeln
raw_dat_motor$MAKE <- toupper(raw_dat_motor$MAKE)

#Entfernen von Duplikaten und Zählen der entfernten Zeilen
removed_count <- nrow(raw_dat_motor) - nrow(raw_dat_motor <- distinct(raw_dat_motor))
#Ausgabe der Anzahl der entfernten Duplikate
cat("Anzahl der entfernten Duplikate:", removed_count, "\n")
# Behalten von Zeilen, in denen MAKE mit Buchstaben beginnt
raw_dat_motor <- raw_dat_motor[grepl("^[A-Za-z]", raw_dat_motor$MAKE), ]
table(raw_dat_motor$MAKE)


#Manuelle Korrekturen von MAKE
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("PORCHE", "FORSCHE")] <- "PORSCHE"
raw_dat_motor$MAKE[raw_dat_motor$MAKE == "YAMHA"] <- "YAMAHA"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("VOLKSWAGON", "VOLKS WAGON")] <- "VOLKSWAGEN"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("TOYOTAA", "TOYOTA*", "TOYTA", "TOYOTA AUTOMOBILE",
                                             "TOYATA", "T0Y0TA", "COMPACT YARIS", "YARIS",
                                             "LAND CRUISER", "VITZ")] <- "TOYOTA"
raw_dat_motor$MAKE[grepl("^TOYOTA", raw_dat_motor$MAKE)] <- "TOYOTA"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("NISAN", "NISSAN*")] <- "NISSAN"
raw_dat_motor$MAKE[grepl("^NISSAN", raw_dat_motor$MAKE)] <- "NISSAN"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("RENGE  ROVER", "RANGEROVER")] <- "RANGE ROVER"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("RENALT", "RENUALT", "RENAULT/STOLARCZYK", "RENAULT*")] <- "RENAULT"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("PEUGEOUT", "PEJOT", "PAGOT")] <- "PEUGEOT"
raw_dat_motor$MAKE[grepl("^PEUGEOT", raw_dat_motor$MAKE)] <- "PEUGEOT"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("BMB")] <- "BMW"
raw_dat_motor$MAKE[grepl("^BMW", raw_dat_motor$MAKE)] <- "BMW"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("MISTIBUSH", "MITSUBISHI*", "MITSUBUSHI")] <- "MITSUBISHI"
raw_dat_motor$MAKE[grepl("^FORD", raw_dat_motor$MAKE)] <- "FORD"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("SPORTAGE")] <- "KIA"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("MERCEEDES", "MERCEEDICE", "MERCEDICE", "MERCHEDES")] <- "MERCEDES"
raw_dat_motor$MAKE[grepl("^MERCEDES", raw_dat_motor$MAKE)] <- "MERCEDES"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("SUZIKE")] <- "SUZUKI"
raw_dat_motor$MAKE[grepl("^SUZUKI", raw_dat_motor$MAKE)] <- "SUZUKI"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("HYUNDI GETZ")] <- "HYUNDAI"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("ISUSU")] <- "ISUZU"
raw_dat_motor$MAKE[grepl("^ISUZU", raw_dat_motor$MAKE)] <- "ISUZU"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("FRANKUN IVECO")] <- "IVECO"
raw_dat_motor$MAKE[grepl("^IVECO", raw_dat_motor$MAKE)] <- "IVECO"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("LANDROVER")] <- "LAND ROVER"


raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("DONGFANG", "DONFING", "DONG FENGSHEN", "DONG FENG")] <- "DONGFENG"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("MAHANDRA")] <- "MAHINDRA"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("HOLAND CAR")] <- "ABAY"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("CALABRASE")] <- "CALABRESE"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("GELYION", "GENLION", "GELION", "GENLYONIVECO", "HONGYAN")] <- "GENLYON"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("DHATSU", "DIATSU", "DIAHATSU")] <- "DAIHATSU"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("DAWOO", "DAWWO", "DEAWOO", "DEAWOO USE")] <- "DAEWOO"
raw_dat_motor$MAKE[grepl("^LIFAN", raw_dat_motor$MAKE)] <- "LIFAN"
raw_dat_motor$MAKE[grepl("^BAIC", raw_dat_motor$MAKE)] <- "BAIC"
raw_dat_motor$MAKE[grepl("^LOADER", raw_dat_motor$MAKE)] <- "LOADER"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("MOTOR CYCLE", "MOTOR  CYCLE")] <- "MOTORCYCLE"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("AUTO", "AUTOMOBIL")] <- "AUTOMOBILE"
raw_dat_motor$MAKE[grepl("^CATERPILLAR", raw_dat_motor$MAKE)] <- "CATERPILLAR"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("LIBER DOZER", "LIBERR MOBILE CRANE",
                                             "LIEBERR MOBILE CRANE", "LEABER CATO CRANE",
                                             "LEBHER CRANE", "CRANE LEBEHER",
                                             "CRANE LEBHERER", "LIBER", "LIBEHER CRANE",
                                             "LIBERR MOBILECRANE", "CRANELEBHER")] <- "LIEBHERR"

#table(raw_dat_motor$MAKE)

#Zähle die Anzahl der Einträge pro MAKE, wo CLAIM_PAID == "YES"
count_claims_paid <- raw_dat_motor %>%
  group_by(MAKE, CLAIM_PAID ) %>%                 # Gruppiere nach MAKE
  summarise(count = n()) %>%        # Zähle die Einträge pro Gruppe
  arrange(desc(count)) 

###############################################################################################


#Liste der gewünschten Fahrzeughersteller (Make)
selected_makes <- c("TOYOTA", "ISUZU", "NISSAN", "IVECO", "SINO HOWO", 
                    "MITSUBISHI", "BISHOFTU", "LIFAN", "FORD", "HYUNDAI", 
                    "MAZDA", "GEELY", "DAEWOO", "MERCEDES", "TATA", 
                    "FIAT", "SINO", "SUZUKI", "GENLYON", "RENAULT")

#Filtern des Datensatzes nach den ausgewählten Fahrzeugherstellern
clean_dat_motor <- subset(raw_dat_motor, MAKE %in% selected_makes)

#Wenige Ausprägungen entfernen
#table(clean_dat_motor$SEX) #OK

#table(clean_dat_motor$USAGE)
clean_dat_motor <- subset(clean_dat_motor, !(USAGE %in% c("Agricultural Any Farm",
                                                          "Agricultural Own Farm",
                                                          "Special Construction", "Taxi")))

#table(clean_dat_motor$TYPE_VEHICLE)
#Wenige Ausprägungen entfernen
clean_dat_motor <- subset(clean_dat_motor, !(TYPE_VEHICLE %in% c("Tractor")))

#table(clean_dat_motor$INSR_TYPE)
#Wenige Ausprägungen entfernen
clean_dat_motor <- subset(clean_dat_motor, !(INSR_TYPE %in% c("Motor trade road risk")))

#table(clean_dat_motor$MAKE)

#CLAIM_PAID
data.frame(
  CLAIM_PAID_0 = sum(clean_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0 = sum(clean_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))

data.frame(
  CLAIM_PAID_0_Percent = 100 * mean(clean_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0_Percent = 100 * mean(clean_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))

#New Variable: age_vehicle
clean_dat_motor$AGE_VEHICLE <- 2018 - clean_dat_motor$PROD_YEAR
#Entfernen von PROD_YEAR
clean_dat_motor<- clean_dat_motor[,-8]



#New variable: AMOUNT_CLAIMS_PAID (Number of previous claims per object ID)
clean_dat_motor <- clean_dat_motor %>%
  # Für jede object_ID und jedes Startjahr, kumuliere die Anzahl vorheriger Claims mit "YES"
  group_by(OBJECT_ID) %>%
  arrange(OBJECT_ID, START_INS_YR) %>%
  mutate(
    AMOUNT_CLAIMS_PAID = sapply(seq_along(START_INS_YR), function(i) {
      sum(CLAIM_PAID[1:(i-1)] == "YES" & START_INS_YR[1:(i-1)] < START_INS_YR[i])
    })
  ) %>%
  ungroup()


###############################################################################################

#Vergleich zum Rohdatensatz
#CLAIM_PAID
data.frame(
  CLAIM_PAID_0 = sum(raw_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0 = sum(raw_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))

data.frame(
  CLAIM_PAID_0_Percent = 100 * mean(raw_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0_Percent = 100 * mean(raw_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))

data.frame(
  CLAIM_PAID_0_Percent = 100 * mean(clean_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0_Percent = 100 * mean(clean_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))


###############################################################################################

#NA
colSums(is.na(raw_dat_motor)) 
colSums(is.na(clean_dat_motor)) 

#Entfernen der Variablen 'versicherungsbeginn' und 'versicherungsende'
clean_dat_motor <- subset(clean_dat_motor, select = -c(INSR_BEGIN, INSR_END, DURATION))

# Neuanordnung der Spalten: Zuerst OBJECT_ID, dann START_INS_YR, dann der Rest
clean_dat_motor <- clean_dat_motor %>%
  dplyr::select(OBJECT_ID, START_INS_YR, SEX, INSR_TYPE, USAGE, TYPE_VEHICLE,
                MAKE, AGE_VEHICLE, SEATS_NUM, CCM_TON, INSURED_VALUE, PREMIUM, CLAIM_PAID_USD, everything())

#Data set too large: Draw 100000 random samples
clean_dat_motor_origin<- clean_dat_motor
set.seed(123)
clean_dat_motor <- sample_n(clean_dat_motor, size = 100000)


#Umwandlung der kategorialen Variablen in Factor-Variablen
clean_dat_motor$OBJECT_ID <- as.factor(clean_dat_motor$OBJECT_ID)
clean_dat_motor$SEX <- as.factor(clean_dat_motor$SEX)
clean_dat_motor$INSR_TYPE <- as.character(clean_dat_motor$INSR_TYPE)
clean_dat_motor$INSR_TYPE <- as.factor(clean_dat_motor$INSR_TYPE)
clean_dat_motor$MAKE <- as.factor(clean_dat_motor$MAKE)
clean_dat_motor$USAGE <- as.factor(clean_dat_motor$USAGE)
clean_dat_motor$CLAIM_PAID <- as.factor(clean_dat_motor$CLAIM_PAID)
clean_dat_motor$TYPE_VEHICLE <- as.factor(clean_dat_motor$TYPE_VEHICLE)
clean_dat_motor$START_INS_YR <- as.factor(clean_dat_motor$START_INS_YR)


# Clear Workspace
rm(clean_dat_motor_origin)


```



# Graphical Data Analysis

First, the distribution of the individual numerical variables is analysed to determine whether any transformations are necessary.

```{r graph_data_analysis, cache = TRUE}

# Auswahl nur der numerischen Variablen
numeric_vars <- clean_dat_motor %>%
  select_if(is.numeric)

# Erstellen von Histogrammen für jede numerische Variable
for (var in names(numeric_vars)) {
  print(
    ggplot(clean_dat_motor, aes_string(x = var)) +
      geom_histogram(bins = 30, color = "black", fill = "skyblue") +
      ggtitle(paste("Histogram of", var)) +
      theme_minimal() +
      xlab(var) +
      ylab("Frequency") +
      theme(plot.title = element_text(hjust = 0.5))
  )
}

###################################################################################

#Weitere grafische Analysen
boxplot(PREMIUM ~ SEX, data = clean_dat_motor,
main = "Premium against sex",
ylab = "Premium")

boxplot(CLAIM_PAID_USD ~ SEX, data = clean_dat_motor,
main = "Claim paid (USD) against sex",
ylab = "Claim paid (USD)")


##############################################

#Log-Transformation
clean_dat_motor$INSURED_VALUE_log <- log(clean_dat_motor$INSURED_VALUE)
clean_dat_motor$PREMIUM_log <- log(clean_dat_motor$PREMIUM)
#Log-Transformation mit log1p() wegen der vielen 0-Werte
clean_dat_motor$CLAIM_PAID_USD_log <- log1p(clean_dat_motor$CLAIM_PAID_USD)
clean_dat_motor$CCM_TON_log <- log1p(clean_dat_motor$CCM_TON)


# Clear Workspace
rm(list = setdiff(ls(), c("clean_dat_motor_origin", "clean_dat_motor")))

```

The histograms show that the variables INSURED_VALUE, PREMIUM, CLAIM_PAID_USD and CCM_TON are right-skewed and require a log transformation. The transformed variables can be inserted in the later regression models instead of the original variables.


# Models
## Linear Model

A linear model is adapted, whereby CLAIM_PAID_USD_log was not included, as the premium is incurred at the start of the contract and this would therefore not make technical sense. Instead, a bonus-malus system is taken into account by adding AMOUNT_CLAIMS_PAID.

```{r linear_model, cache = TRUE}

#Fit the linear model
lm_model <- lm(PREMIUM_log ~ SEX + INSR_TYPE + USAGE + TYPE_VEHICLE + MAKE +
                 AGE_VEHICLE + SEATS_NUM + CCM_TON_log +INSURED_VALUE_log +
                 AMOUNT_CLAIMS_PAID, data = clean_dat_motor)

#Summary of the model
summary(lm_model)
#Perform an F-test to drop non-significant categorical variables one at a time
drop1(lm_model, test= "F") #Bei mehrere Kat Var.
#coefficients
coef(lm_model)

#Variance Inflation Factor (VIF) for each predictor to check for multicollinearity
#vif(lm_model)

#Residual Analysis
#Q-Q Plot to assess if residuals follow a normal distribution
qqnorm(lm_model$residuals)
qqline(lm_model$residuals, col = "red")

#Residuals vs Fitted Values (check for patterns or heteroscedasticity)
plot(lm_model$fitted.values, lm_model$residuals, 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted Values",
     ylab = "Residuals")
abline(h = 0, col = "red")

#Model Performance
#Calculation of Mean Squared Error (MSE)
mse <- mean(lm_model$residuals^2)
cat("Mean Squared Error (MSE):", mse, "\n")

#Calculation of R-squared value
r_squared <- summary(lm_model)$r.squared
cat("R-squared:", r_squared, "\n")

```


The model summary shows that the Multiple R-squared value is 0.7308, indicating that the model can explain approximately 73.08% of the variance in premiums. This suggests that the model provides a good fit to the data. The F-test for the overall model is significant (p < 2.2e-16), indicating that the predictors as a group have a substantial effect on the premium.

All predictors have a significant impact on the target variable PREMIUM_log. For instance, the categories SEX and USAGE (usage) have a significant effect on PREMIUM_log. Men pay slightly less compared to women, while certain usages, such as "Fare Paying Passengers," lead to higher premiums. In contrast, usages like "Own Goods" and "Private" are associated with lower premiums.

The coefficient of INSURED_VALUE_log (0.7682) in the model shows that the insured value of the vehicle has a strong influence on the premium level. Since both the insured value and the premium are logarithmically transformed, this means that a 1% increase in the insured value results in approximately a 0.7682% increase in the premium. This illustrates the direct and positive relationship between vehicle value and premium: higher-insured vehicles attract proportionally higher premiums, as they represent a greater financial risk for the insurer. Overall, this coefficient confirms that vehicle value is one of the most significant factors in premium calculation.

The coefficient of AMOUNT_CLAIMS_PAID, with a value of 0.1363, indicates that an increase in the number of claims leads to an increase in the log-transformed premium by approximately 0.1363. This means that each additional claim results in a proportional increase in the premium by about 13.63%. This coefficient highlights that an insured’s claim history has a significant impact on the premium level.

The coefficient of AGE_VEHICLE is 0.0029, indicating that with each additional year of vehicle age, the log-transformed premium increases by about 0.0029. Since the target variable is logarithmic, this implies that an additional year in vehicle age leads to a minimal increase in the premium of approximately 0.29%.

The coefficient of SEATS_NUM is -0.00175, which means that with each additional seat, the log-transformed premium decreases by approximately 0.00175. Given the logarithmic nature of the target variable, this can be interpreted as each additional seat leading to a slight reduction in the premium by around 0.175%.


VIF:
An analysis of multicollinearity revealed that the Variance Inflation Factor (VIF) for the variable INSR_TYPE is 5.85, which suggests possible multicollinearity. This could affect the model’s stability and interpretability and should be considered in further model optimization.


Residuals Analysis
Residuals vs. Fitted Plot:
The Residuals vs. Fitted Plot displays a funnel-shaped pattern, indicating heteroskedasticity. The variance of the residuals increases with higher predicted values, meaning that the model is less accurate for larger premium values. This violates the assumption of constant variance, suggesting that homoskedasticity is not fully met.

Normal Q-Q Plot:
The Normal Q-Q Plot shows that the residuals do not lie perfectly along the line, indicating significant deviations from the theoretical normal distribution, particularly at the tails. These "heavy tails" suggest a non-normal distribution of residuals, potentially due to outliers or unmodeled non-linear relationships.

To improve the model, various measures could be considered. One approach would be to transform the target variable, for example, using a Box-Cox transformation, to reduce heteroskedasticity and achieve a more stable residual variance. Additionally, incorporating non-linear relationships by including polynomial terms or using a generalized linear model (GLM) could be beneficial. This would allow the model to better capture complex relationships between variables, thereby enhancing predictive accuracy.


## Poisson

A Poisson model is fitted to predict the number of claims over a 5-year period based on the characteristics SEX, INSR_TYPE, USAGE, TYPE_VEHICLE, MAKE, AGE_VEHICLE, SEATS_NUM, CCM_TON, INSURED_VALUE, and PREMIUM.

First, the data is grouped accordingly, and the results are analyzed to gather insights.

```{r data_for_poisson, cache=TRUE}

#Data preparation: Aggregate the number of claims per combination
dat_amount_claims <- clean_dat_motor %>%
  group_by(SEX, INSR_TYPE, USAGE, TYPE_VEHICLE, MAKE, AGE_VEHICLE, SEATS_NUM, CCM_TON, INSURED_VALUE, PREMIUM) %>%
  summarise(AMOUNT_CLAIMS = sum(CLAIM_PAID == "YES"), .groups = 'drop')

summary(dat_amount_claims$AMOUNT_CLAIMS)
hist(dat_amount_claims$AMOUNT_CLAIMS, breaks=max(dat_amount_claims$AMOUNT_CLAIMS)) #Zero-inflated Poisson Regression would fit better

mean(dat_amount_claims$AMOUNT_CLAIMS) # calculate mean
var(dat_amount_claims$AMOUNT_CLAIMS)
#The variance is not much greater than the mean, over-dispersion can be neglegted in the model.

```
The analysis of the distribution of the target variable AMOUNT_CLAIMS reveals that a large portion of the values are zero. This concentration of zero values is confirmed by the median, as well as the 1st and 3rd quartiles, which are also at zero. Additionally, the distribution shows some high outliers with a maximum value of 46, indicating an uneven distribution with a few high values. The low mean of 0.1791 further supports this observation, suggesting a significant number of zero values.Given these distribution characteristics, the use of a Zero-Inflated Poisson (ZIP) model could be appropriate, as such a model can account for both random and structural zeros. Initially, however, a Poisson model will be fitted.

```{r Poisson_Model, cache=TRUE}

#Poisson Regression:
poisson_model<- glm(AMOUNT_CLAIMS ~ SEX + INSR_TYPE + TYPE_VEHICLE +  MAKE +
                      AGE_VEHICLE + SEATS_NUM + CCM_TON + INSURED_VALUE + PREMIUM,
                      family = poisson(link = "log"), data = dat_amount_claims)

#Summary of the model
summary(poisson_model)

#Model coefficients and exponentiated coefficients (Rate Ratios)
coef(poisson_model)
exp(coef(poisson_model))

# Diagnose Overdispersion
overdispersion <- deviance(poisson_model) / df.residual(poisson_model)
# Interpretation:
# - A value close to 1 indicates an adequate fit without overdispersion.
# - Values significantly greater than 1 suggest overdispersion, indicating that data variance exceeds the model's assumptions.
#   If overdispersion is present, consider using a Negative Binomial model for a better fit.
cat("Overdispersion ratio (Deviance / DF):", overdispersion, "\n")

# Poisson Deviance Test: Check Goodness-of-Fit of the model
# Calculate p-value for H0: The model adequately describes the data
p_value_fit <- 1 - pchisq(deviance(poisson_model), df.residual(poisson_model))
cat("Goodness-of-Fit p-value:", p_value_fit, "\n")
# Interpretation:
# - A high p-value (> 0.05) suggests a good model fit to the data, as we do not reject H0.
# - A low p-value (≤ 0.05) would indicate a poor fit, suggesting that the model does not describe the data well.

# Calculate VIF values for all predictor variables in the model
vif_values <- vif(poisson_model)

# Output the VIF values for interpretation
cat("Variance Inflation Factor (VIF) values for all predictor variables:\n")
print(vif_values)
#Interpretation:
# - VIF values above 5 (or in some cases, 10) indicate multicollinearity issues.


# Berechnung der vorhergesagten Werte für Visualisierung
poisson_model$model$fitted <- predict(poisson_model, type = "response")
#ggplot2 Plot: Fitted vs. Actual values
ggplot(poisson_model$model) +
    geom_point(aes(x = fitted, y = AMOUNT_CLAIMS), color = "darkblue") +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") + # Diagonale Linie als Referenz
    labs(x = "Fitted Values \n", y = "Actual number Of Claims \n",
         title = "Poisson Regression: Fitted vs. Actual number of Claims \n") +
    theme(plot.title = element_text(hjust = 0.5),
          axis.title.x = element_text(face="bold", colour="blue", size = 12),
          axis.title.y = element_text(face="bold", colour="blue", size = 12))



##Poisson: (Profile-Likelihood-Vertrauensintervalle)
#confint(poisson_model)
##Poisson: (Wald Vertrauensintervall)
#coef(poisson_model)["F"]
#c(-1, 1) * qnorm(1 - 0.95/2) * summary(poisson_model)$coefficients["F", "Std. Error"]
##Poisson: Punktschaetzung, 95% Vertrauensintervall
#pred<- predict(herz.fit, se.fit= TRUE, type= "link")
#pe<- exp(pred$fit)
#lb<- exp(pred$fit + qnorm(1 - 0.05 / 2) * pred$se.fit) #untere Schranke
#ub<- exp(pred$fit - qnorm(1 - 0.05 / 2) * pred$se.fit) #obere Schranke
#rbind(lb, ub)

#Poisson: Inferenz
#Signifikanztest der einzelnen Parameter

```
The analysis of the Poisson model for predicting claim frequency indicated no overdispersion. The calculated overdispersion value, represented by the ratio of deviance to degrees of freedom, is 0.688, which is significantly below 1. This suggests that the model does not overestimate variance in the data, and overdispersion is not an issue. The Goodness-of-Fit test further confirms the adequacy of the model, as the p-value of 1 supports the null hypothesis that the model sufficiently describes the data.

The Poisson regression model for predicting the number of claims reveals that several variables show statistically significant relationships with claim frequency. The model indicates statistically significant differences in claim frequency across categories (p-value < 0.001). The group of legal entities, which serves as the reference category, exhibits the highest claim rate. Compared to legal entities, males have a rate ratio of 0.666, reflecting a 33.4% lower claim rate, while females have the lowest claim frequency, with a rate ratio of 0.623, or 37.7% below that of legal entities.

For the insurance type (INSR_TYPE), it was found that INSR_TYPEPrivate has a rate ratio of 1.284, indicating that private insurers have a 28.4% higher claim probability compared to the reference category INSR_TYPECommercial. The variables TYPE_VEHICLE and MAKE also show significant differences in claim rates. Among vehicle types, Pick-up has the highest claim rate, with a rate ratio of 1.121, representing a 12.1% increase in claim probability compared to the reference category Automobile; however, this effect is not statistically significant (p-value = 0.120). Conversely, Motor-cycle has the lowest claim rate, with a rate ratio of 0.039, indicating an approximately 96% reduced claim probability and a highly significant result (p-value < 0.001).

Among vehicle brands, GEELY shows the highest claim rate with a rate ratio of 1.040, which, however, represents no meaningful change compared to the reference brand BISHOFTU and is statistically insignificant (p-value = 0.709). Conversely, MERCEDES has the lowest claim rate, with a rate ratio of 0.403, indicating a 59.7% lower claim probability compared to BISHOFTU and is highly significant (p-value < 0.001).

These results suggest that Pick-up and GEELY exhibit the highest, though statistically insignificant, claim rates, while Motor-cycle and MERCEDES show the lowest and statistically significant claim rates relative to their respective reference categories.

Further analysis indicates that vehicle age (AGE_VEHICLE) has a rate ratio of 0.956, meaning that the claim rate decreases by approximately 4.4% with each additional year (p-value < 0.001). The number of seats (SEATS_NUM) shows a rate ratio of 1.009, indicating that each additional seat slightly increases the claim probability, though significantly. Engine capacity (CCM_TON) shows no practical change in claim rate with a rate ratio of 1.000031, though it is statistically significant (p-value < 0.001). Insured value (INSURED_VALUE) has a rate ratio of 0.99999986, effectively showing no influence on claim frequency, although the effect is statistically significant. Premium amount (PREMIUM) exhibits a rate ratio of 1.000019, suggesting a minimal increase in claim probability with rising premiums; again, the effect is significant but very small.

The analysis of the Poisson model reveals significant multicollinearity, reflected in extremely high VIF values for some variables. Notably, the variables TYPE_VEHICLEMotor-cycle (VIF of 200.88), TYPE_VEHICLETruck (107.23), TYPE_VEHICLEPick-up (82.92), INSR_TYPEPrivate (80.14), and MAKETOYOTA (50.55) stand out. These high values indicate that these variables are highly correlated with other predictors, especially among the vehicle type variables, suggesting redundancy within the model. Other variables, such as CCM_TON (33.28), MAKEISUZU (28.86), MAKEIVECO (23.94), and MAKETATA (30.58), also display moderate multicollinearity, while some variables, like MAKEGEELY (3.99), show lower VIF values and are less strongly correlated with other predictors.

The plots of estimated vs. actual values show that the Poisson model has difficulties in accurately modelling the distribution of claims, especially for higher claims values. Most of the predicted values are close to zero and systematically underestimate the actual loss frequencies as they increase. This systematic underestimation and the high number of zero claims indicate that the simple distribution of the Poisson model may not be sufficient to fully represent the structure of the data.

Given the high number of zero values in the data, a Zero-Inflated Poisson (ZIP) model could represent a useful alternative. Such a model can distinguish between structural zeros (cases where no claims occur) and random zeros (cases where claims could occur but did not), potentially improving predictive accuracy for higher claim counts without violating model assumptions about variance.

As a further alternative, simplifying the model, for example by removing fewer significant variables, could be a sensible measure to improve the model.


### Massnahme 1): Zero-Inflated Poisson (ZIP) model (eventuell weglassen, machts naemlich ned besser)

```{r zero_inflated_pois_model, cache=TRUE}

# Anpassung eines Zero-Inflated Poisson (ZIP)-Modells
zip_model <- zeroinfl(AMOUNT_CLAIMS ~ SEX + INSR_TYPE + TYPE_VEHICLE + MAKE +
                        AGE_VEHICLE + SEATS_NUM + CCM_TON + INSURED_VALUE + PREMIUM,
                      data = dat_amount_claims, dist = "poisson")

# Zusammenfassung des Modells
summary(zip_model)

# Überprüfung der Koeffizienten und ihrer Exponentialwerte (Rate Ratios)
coef(zip_model)
exp(coef(zip_model))


# Calculate VIF values for all predictor variables in the model
vif_values_zip <- vif(zip_model)

# Output the VIF values for interpretation
cat("Variance Inflation Factor (VIF) values for all predictor variables:\n")
print(vif_values_zip)
#Interpretation:
# - VIF values above 5 (or in some cases, 10) indicate multicollinearity issues.


# Diagnose der Modellanpassung (angepasste vs. tatsächliche Werte)
# Berechnung der vorhergesagten Werte
dat_amount_claims$fitted_zip <- predict(zip_model, type = "response")

# Plot der geschätzten vs. tatsächlichen Schadensfälle
ggplot(dat_amount_claims, aes(x = fitted_zip, y = AMOUNT_CLAIMS)) +
  geom_point(color = "darkblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values (ZIP)", y = "Actual Number of Claims",
       title = "Zero-Inflated Poisson: Fitted vs. Actual Number of Claims") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_text(face = "bold", colour = "blue", size = 12),
        axis.title.y = element_text(face = "bold", colour = "blue", size = 12))

```


## Binomial
# TODO

```{r Binomial_Model, cache=TRUE}

#Anpassen eines logistischen Regressionsmodells
fit.binom <- glm(CLAIM_PAID ~ SEX + INSR_TYPE +  USAGE + TYPE_VEHICLE +
             MAKE + AGE_VEHICLE + SEATS_NUM + CCM_TON_log + INSURED_VALUE_log +
               PREMIUM_log + AMOUNT_CLAIMS_PAID,
             data = clean_dat_motor, 
             family = binomial(link= "logit"))

#Inferenz und Signifikanztests
summary(fit.binom)  # Wald-Test zur Überprüfung der Signifikanz einzelner Koeffizienten

#Likelihood-Ratio-Test (LRT) für die Signifikanz einzelner Variablen
drop1(fit.binom, test = "LRT")

#Berechnung AIC (Modellgütebewertung)
aic_logit_binom <- AIC(fit.binom)
cat("AIC des logit-Modells:", aic_logit_binom, "\n")

#Modellgüte: Berechnung von Pseudo-R^2-Werten mit Nagelkerke für Modellbewertung
nagelkerke(fit.binom)$Pseudo.R.squared.for.model.vs.null
#McFadden's Pseudo R^2: 0.033 3.4%
#Nagelkerke: 0.048 -> 4.8%
#Entsprechend erkl?rt das Modell ca.3.4% der Devianz (McFadden)
#bzw. 4.84% der Varianz (Nagelkerke). schlechte Ergebnisse

# Berechnung des globalen F-Tests zur Überprüfung der Signifikanz des gesamten Modells
TD <- fit.binom$null.deviance - fit.binom$deviance  # Teststatistik
df_diff <- fit.binom$df.null - fit.binom$df.residual  # Freiheitsgrade
p_value <- 1 - pchisq(TD, df = df_diff)  # p-Wert für den F-Test
cat("Globale Teststatistik (TD):", TD, "\n")
cat("p-Wert des globalen Tests:", p_value, "\n")

# Berechnung der Deviance-Residuals zur Überprüfung der Anpassung
deviance_residuals <- residuals(fit.binom, type = "deviance")
cat("Zusammenfassung der Deviance-Residuals:\n")
summary(deviance_residuals)

# Überprüfung der Overdispersion im Modell
deviance <- deviance(fit.binom)
df_residual <- df.residual(fit.binom)
overdispersion_ratio <- deviance / df_residual
cat("Overdispersion Ratio (Deviance / DF):", overdispersion_ratio, "\n")

# Globale Modellüberprüfung durch Likelihood-Ratio-Test gegen das Nullmodell
null_model <- glm(CLAIM_PAID ~ 1, data = clean_dat_motor, family = binomial(link = "logit"))
lrt_statistic <- 2 * (logLik(fit.binom) - logLik(null_model))
cat("Likelihood-Ratio-Test-Statistik gegen Nullmodell:", lrt_statistic, "\n")

# ROC-Kurve und AUC zur Modellbewertung
library(pROC)  # Falls noch nicht geladen
predicted_probabilities <- predict(fit.binom, type = "response")
roc_curve <- roc(clean_dat_motor$CLAIM_PAID, predicted_probabilities)
auc_value <- auc(roc_curve)
cat("AUC-Wert des Modells:", auc_value, "\n")

# Visualisierung der ROC-Kurve
plot(roc_curve, main = "ROC-Kurve für logistische Regression", col = "blue")






##Residuenanalyse?
#par(mfrow= c(2, 3), mar= c(3.5, 3.5, 2, 1), mgp= c(2.25, 1, 0))
#termplot(fit.binom, partial= TRUE, smooth= panel.smooth,
#         col.term= "black", col.smth= "red")
##Partielle Resiudenplotz zeigen Verst?sse geg. Linearit?t, Var. ph zeigt sogar  quadr. Effekt
##Output loschen
#plot(fit.binom, which= 5)
#
#
## Wahrscheinlichkeiten vorhersagen
#clean_dat_motor_undersampled$predicted_probabilities <- predict(fit.binom_undersampled2, type = "response")
#
## Klassifikation vorhersagen (0 oder 1)
#clean_dat_motor_undersampled$predicted_class <- ifelse(clean_dat_motor_undersampled$predicted_probabilities > 0.5, 1, 0)
#
## Modellgüte prüfen
#table(clean_dat_motor_undersampled$CLAIM_PAID, clean_dat_motor_undersampled$predicted_class)
#
## Genauigkeit berechnen
#accuracy_undersampled <- mean(clean_dat_motor_undersampled$CLAIM_PAID == clean_dat_motor_undersampled$predicted_class)
#print(paste("Genauigkeit des undersampelten Modells:", accuracy_undersampled))
#
#
#
#
## Umwandlung in Faktoren für die Auswertung
#predicted_class <- as.factor(clean_dat_motor_undersampled$predicted_class)
#actual_class <- as.factor(clean_dat_motor_undersampled$CLAIM_PAID)
#
#
## Sicherstellen, dass predicted_class und actual_class die gleichen Levels haben
#levels(predicted_class) <- levels(actual_class)
#
## Levels überprüfen
#levels(predicted_class)
#levels(actual_class)
#
## Berechnung der Metriken
#confusionMatrix(predicted_class, actual_class)


```

Modellgüte
Die Berechnung von Pseudo-R²-Werten ergab niedrige Werte (McFadden: 3,4% und Nagelkerke: 4,8%). Diese niedrigen Werte zeigen, dass das Modell nur einen kleinen Teil der Varianz in der Zielvariablen CLAIM_PAID erklären kann. Es handelt sich also um ein Modell mit begrenzter Erklärungskraft.

Die Modellgüte wurde mittels Pseudo-R²-Werten bewertet. Der McFadden-Wert (0.0337) und der Nagelkerke-Wert (0.0484) sind relativ niedrig und deuten darauf hin, dass das Modell nur einen kleinen Anteil der Varianz in der Zielvariablen erklärt. Diese Werte deuten darauf hin, dass das Modell nur begrenzte Vorhersagekraft besitzt und möglicherweise noch wichtige Prädiktoren fehlen oder die Variablenkategorien weitere Anpassungen benötigen.


Globale Modellsignifikanz:
Der globale F-Test (Wald-Test) zeigt, dass die Gesamtmodellstatistik (TD) 2709.95 beträgt, und der p-Wert des Tests ist nahe null. Dies deutet darauf hin, dass das Modell als Ganzes statistisch signifikant ist. Das bedeutet, dass zumindest eine der erklärenden Variablen in signifikantem Zusammenhang mit der Zielvariablen steht und somit das Modell besser ist als ein Nullmodell (Modell ohne erklärende Variablen).

Ergebnisse des Likelihood-Ratio-Tests
Gemäss dem Likelihood-Ratio-Test, scheint (INSR_TYPE) keinen signifikanten Einfluss zu haben.



Der AUC-Wert (Fläche unter der Kurve) von 0,629 deutet auf eine relativ moderate Diskriminierungsfähigkeit hin, was bedeutet, dass das Modell etwas besser als zufälliges Raten ist, aber noch Raum für Verbesserungen bietet.


Modellanpassung und Signifikanz:
Der AIC (Akaike Informationskriterium) des Modells beträgt 77716,2. Ein niedrigerer AIC-Wert zeigt in der Regel eine bessere Anpassung an.
Das Overdispersion-Verhältnis beträgt 0,7766, was unter 1 liegt und auf keine signifikante Overdispersion hinweist.
Mehrere Prädiktoren sind statistisch signifikant (z.B. SEX, TYPE_VEHICLE, MAKE, PREMIUM_log, usw.).

Likelihood-Ratio-Test:

Der Likelihood-Ratio-Test gegen das Nullmodell ergibt einen Wert von 2709,954, was darauf hinweist, dass das vollständige Modell signifikant besser ist als das Nullmodell.

ROC und AUC:
Die ROC-Kurve zeigt den Kompromiss zwischen Sensitivität und Spezifität.
Der AUC-Wert beträgt 0,629, was auf eine gewisse Vorhersagekraft des Modells hindeutet, jedoch möglicherweise weitere Optimierungen erfordert.

Pseudo R-Quadrat:
Der McFadden-Pseudo-R-Quadrat beträgt 0,0337, was auf eine moderate Anpassung hinweist, während der Nagelkerke-Pseudo-R-Quadrat 0,0484 beträgt, was ebenfalls eine begrenzte Erklärungskraft des Modells andeutet.


### Massnahme 1): Entfernen von Variablen

```{r Binomial_Model_sign, cache=TRUE, eval=FALSE}

# Neues Modell ohne INSR_TYPE
fit.binom.sign <- glm(
  CLAIM_PAID ~ SEX + USAGE + TYPE_VEHICLE + MAKE + AGE_VEHICLE + SEATS_NUM +
               CCM_TON_log + INSURED_VALUE_log + PREMIUM_log + AMOUNT_CLAIMS_PAID,
  data = clean_dat_motor,
  family = binomial(link = "logit")
)

# Zusammenfassung des neuen Modells anzeigen (Signifikanz einzelner Variablen)
summary(fit.binom.sign)

#Likelihood-Ratio-Test (LRT) für die Signifikanz einzelner Variablen
drop1(fit.binom.sign, test = "LRT")


anova(fit.binom, fit.binom.sign, test= "LRT")

# AIC des neuen Modells berechnen und ausgeben
aic_no_insr_seats <- AIC(fit.binom.sign)
cat("AIC des Modells ohne INSR_TYPE:", aic_no_insr_seats, "\n")

# Vergleich des AIC-Wertes mit dem ursprünglichen Modell
aic_difference <- aic_logit_binom - aic_no_insr_seats
cat("AIC-Unterschied zum ursprünglichen Modell:", aic_difference, "\n")

# Berechnung des globalen F-Tests zur Überprüfung der Signifikanz des neuen Modells
TD_no_insr_seats <- fit.binom.sign$null.deviance - fit.binom.sign$deviance  # Teststatistik
df_diff_no_insr_seats <- fit.binom.sign$df.null - fit.binom.sign$df.residual  # Freiheitsgrade
p_value_no_insr_seats <- 1 - pchisq(TD_no_insr_seats, df = df_diff_no_insr_seats)  # p-Wert für den F-Test

cat("Globale Teststatistik (TD) des neuen Modells:", TD_no_insr_seats, "\n")
cat("p-Wert des globalen Tests des neuen Modells:", p_value_no_insr_seats, "\n")

# Vergleich der Pseudo-R^2-Werte (Modellgüte) mit Nagelkerke
nagelkerke_new <- nagelkerke(fit.binom.sign)$Pseudo.R.squared.for.model.vs.null
print(nagelkerke_new)

# Berechnung der Deviance-Residuals zur Überprüfung der Anpassung
deviance_residuals <- residuals(fit.binom.sign, type = "deviance")
cat("Zusammenfassung der Deviance-Residuals:\n")
summary(deviance_residuals)

# Überprüfung der Overdispersion im Modell
deviance <- deviance(fit.binom.sign)
df_residual <- df.residual(fit.binom.sign)
overdispersion_ratio <- deviance / df_residual
cat("Overdispersion Ratio (Deviance / DF):", overdispersion_ratio, "\n")

# Globale Modellüberprüfung durch Likelihood-Ratio-Test gegen das Nullmodell
null_model <- glm(CLAIM_PAID ~ 1, data = clean_dat_motor, family = binomial(link = "logit"))
lrt_statistic <- 2 * (logLik(fit.binom.sign) - logLik(null_model))
cat("Likelihood-Ratio-Test-Statistik gegen Nullmodell:", lrt_statistic, "\n")

# ROC-Kurve und AUC zur Modellbewertung
library(pROC)  # Falls noch nicht geladen
predicted_probabilities <- predict(fit.binom.sign, type = "response")
roc_curve <- roc(clean_dat_motor$CLAIM_PAID, predicted_probabilities)
auc_value <- auc(roc_curve)
cat("AUC-Wert des Modells:", auc_value, "\n")

# Visualisierung der ROC-Kurve
plot(roc_curve, main = "ROC-Kurve für logistische Regression", col = "blue")



#ROC mit Testdaten
#Ist Modell besser als zuf?llige Sch?tzung?
library(ROCR)
pred<- prediction(yhat_new, credittest$Creditability) #(Vorhersage, echte ZV)
perf<- performance(pred, "tpr", "fpr")
plot(perf, lwd= 3)
abline(a= 0, b= 1, col= "blue", lwd= 2)
#Je gr?sser Gap / Berg ?ber Diagonaler, desto besser


# Wahrscheinlichkeiten vorhersagen
clean_dat_motor_undersampled$predicted_probabilities <- predict(fit.binom_undersampled2, type = "response")

# Klassifikation vorhersagen (0 oder 1)
clean_dat_motor_undersampled$predicted_class <- ifelse(clean_dat_motor_undersampled$predicted_probabilities > 0.5, 1, 0)

# Modellgüte prüfen
table(clean_dat_motor_undersampled$CLAIM_PAID, clean_dat_motor_undersampled$predicted_class)

# Genauigkeit berechnen
accuracy_undersampled <- mean(clean_dat_motor_undersampled$CLAIM_PAID == clean_dat_motor_undersampled$predicted_class)
print(paste("Genauigkeit des undersampelten Modells:", accuracy_undersampled))




# Umwandlung in Faktoren für die Auswertung
predicted_class <- as.factor(clean_dat_motor_undersampled$predicted_class)
actual_class <- as.factor(clean_dat_motor_undersampled$CLAIM_PAID)


# Sicherstellen, dass predicted_class und actual_class die gleichen Levels haben
levels(predicted_class) <- levels(actual_class)

# Levels überprüfen
levels(predicted_class)
levels(actual_class)

# Berechnung der Metriken
confusionMatrix(predicted_class, actual_class)
```

AIC-Wert: Der AIC des neuen logistischen Modells beträgt 77714.2, was einen minimalen Rückgang gegenüber dem vorherigen Modell darstellt, aber die Anpassung nicht wesentlich beeinflusst.

Zusammengefasst hat das Entfernen der Variable INSR_TYPE nur geringe Auswirkungen auf die Modellgüte und die Erklärungskraft. Falls gewünscht, können weitere Anpassungen oder zusätzliche Prädiktoren in Betracht gezogen werden, um die Modellleistung zu verbessern.



## Generalised Additive Model (GAM)
# TODO description
```{r GAM, cache=TRUE}
# Convert unordered factors to factors
dat_amount_claims$SEX <- as.factor(dat_amount_claims$SEX)
dat_amount_claims$INSR_TYPE <- as.factor(dat_amount_claims$INSR_TYPE)
dat_amount_claims$USAGE <- as.factor(dat_amount_claims$USAGE)
dat_amount_claims$TYPE_VEHICLE <- as.factor(dat_amount_claims$TYPE_VEHICLE)
dat_amount_claims$MAKE <- as.factor(dat_amount_claims$MAKE)

# Fit a Generalized Additive Model (GAM) to the data
gam_model <- mgcv::gam(
  AMOUNT_CLAIMS ~ SEX + INSR_TYPE + USAGE + TYPE_VEHICLE + MAKE +
    s(AGE_VEHICLE, bs = "cr") +
    s(SEATS_NUM, bs = "cr") +
    s(CCM_TON, bs = "cr") +
    s(INSURED_VALUE, bs = "cr") +
    s(PREMIUM, bs = "cr"),
  data = dat_amount_claims,
  family = poisson(link = "log")
)

# Summary of the GAM model
summary(gam_model)

# Visualize the GAM model
plot(gam_model, shade = TRUE, scale = 0)

# Predict on the data for visualization
dat_amount_claims$predicted_claims <- predict(gam_model, 
                                              newdata = dat_amount_claims, 
                                              type = "response")

# Plot the predicted vs. actual claims
ggplot(dat_amount_claims, aes(x = predicted_claims, y = AMOUNT_CLAIMS)) +
  geom_point(color = "darkblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Predicted Claims", y = "Actual Claims",
       title = "GAM: Predicted vs. Actual Claims") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_text(face = "bold", colour = "blue", size = 12),
        axis.title.y = element_text(face = "bold", colour = "blue", size = 12))

# Calculate the deviance of the GAM model
deviance_gam <- deviance(gam_model)
cat("Deviance of the GAM model:", deviance_gam, "\n")

# Calculate the AIC of the GAM model
aic_gam <- AIC(gam_model)
cat("AIC of the GAM model:", aic_gam, "\n")

# Calculate the pseudo-R-squared values for the GAM model
nagelkerke(gam_model)$Pseudo.R.squared.for.model.vs.null

# Check for overdispersion in the GAM model
overdispersion_gam <- deviance_gam / df.residual(gam_model)
cat("Overdispersion ratio (Deviance / DF) for the GAM model:", 
    overdispersion_gam, "\n")

# Calculate the p-value for the GAM model
p_value_gam <- 1 - pchisq(deviance_gam, df.residual(gam_model))
cat("p-value for the GAM model:", p_value_gam, "\n")



```

## Neural Network

A neural network model is fitted to predict the premium amount based on the characteristics of the insured vehicles and the driver. The model is trained using the cleaned and transformed data, and the results are analyzed to evaluate the model's performance.


```{r Neural Network, results='hide'}
# Data Preparation: Split the data into training and testing sets
set.seed(123)
train_index <- sample(seq_len(nrow(clean_dat_motor)), 0.8 * nrow(clean_dat_motor))
train_data <- clean_dat_motor[train_index, ]
test_data <- clean_dat_motor[-train_index, ]

# Scale numeric features for better performance (important for nnet models)
scale_features <- function(data) {
  data$AGE_VEHICLE <- scale(data$AGE_VEHICLE)
  data$SEATS_NUM <- scale(data$SEATS_NUM)
  data$CCM_TON_log <- scale(data$CCM_TON_log)
  data$INSURED_VALUE_log <- scale(data$INSURED_VALUE_log)
  data$AMOUNT_CLAIMS_PAID <- scale(data$AMOUNT_CLAIMS_PAID)
  return(data)
}

train_data <- scale_features(train_data)
test_data <- scale_features(test_data)

# Check if the model already exists
model_path <- "nn_model.rds"
if (file.exists(model_path)) {
  nn_model <- readRDS(model_path)
  cat("Neural network model loaded from", model_path, "\n")
} else {
  # Fit the neural network model using nnet
  nn_model <- nnet(
    PREMIUM_log ~ .,
    data = train_data,
    size = 5,           # Number of neurons in the hidden layer
    linout = TRUE,      # Linear output for regression tasks
    maxit = 100,         # Max iterations for training
    trace = FALSE       # Suppress training output
  )
  
  # Save the neural network model as an RDS file
  saveRDS(nn_model, model_path)
  cat("Neural network model saved as", model_path, "\n")
}

nn_model

# visualize the neural network model
plotnet(nn_model, alpha = 0.5, margin = 0.1, cex = 0.8)
# Predict on the test data
test_data$predicted_premium_log <- predict(nn_model, test_data)

# Convert predicted log values back to the original scale
test_data$predicted_premium <- exp(test_data$predicted_premium_log)
test_data$PREMIUM <- exp(test_data$PREMIUM_log)


# Visualize Predicted vs. Actual Premium (Original Scale)
ggplot(test_data, aes(x = PREMIUM, y = predicted_premium)) +
  geom_point(color = "darkblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Actual Premium",
       y = "Predicted Premium",
       title = "Predicted vs. Actual Premium") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_text(face = "bold", colour = "blue", size = 12),
        axis.title.y = element_text(face = "bold", colour = "blue", size = 12))

# Save the neural network model as an RDS file
model_path <- "nn_model.rds"
if (!file.exists(model_path)) {
  saveRDS(nn_model, model_path)
} else {
}

# Evaluate the model
mse <- mean((test_data$PREMIUM_log - test_data$predicted_premium_log)^2)

# accuracy metrics
r_squared <- 1 - mse / var(test_data$PREMIUM_log)

rmse <- sqrt(mse)

mae <- mean(abs(test_data$PREMIUM_log - test_data$predicted_premium_log))

mape <- mean(abs(test_data$PREMIUM_log - test_data$predicted_premium_log) / test_data$PREMIUM_log) * 100

# make a table of the results
results <- data.frame(Model = "Neural Network",
                      MSE = mse,
                      R_squared = r_squared,
                      RMSE = rmse,
                      MAE = mae,
                      MAPE = mape)

#results
```

The neural network model was trained to predict the log-transformed premium amount based on the characteristics of the insured vehicles. The model was fitted using the nnet package, with the training data split into 80% training and 20% testing sets. The neural network model was trained with a hidden layer size of 5 neurons, linear output for regression tasks, and a maximum of 100 iterations for training. 

The plots of predicted vs. actual premium amounts show that the neural network model generally performs well in predicting the premium amounts. The points are clustered around the diagonal line, indicating a good alignment between the actual and predicted values. The model captures the general trend of the premiums, with some deviations for higher premium values. The model's performance can be further evaluated by considering additional metrics such as the R-squared value, RMSE, MAE, and MAPE, which provide insights into the model's accuracy and predictive power.

### Neural Network Cross Validation

Nevertheless, we cannot be sure that those values for the model above are truly correct or it was luck that the model performs well at a first instance. To solve this question, the NN will be run again using **k-fold Cross Validation** with hyperparameter tuning. This approach will help ensure that the model's performance is robust and not due to overfitting or random chance. The k-fold Cross Validation will produce a more reliable estimate of the model's performance by splitting the data into k = 10 subsets, training the model on k-1 subsets, and validating it on the remaining subset. This process is repeated k times, and the results are averaged to provide a comprehensive evaluation of the model.

```{r Neural Network Cross Validation, results='hide', cache=TRUE}
# Repeat neural network with caret and cross validation
set.seed(123)
# Define the control function for cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Define the grid of hyperparameters to tune
tune_grid <- expand.grid(size = c(3, 5, 7), decay = c(0, 0.1, 0.01))

# Check if the model already exists
model_path_cv <- "nn_model_cv.rds"
if (file.exists(model_path_cv)) {
  nn_model_cv <- readRDS(model_path_cv)
} else {
  # Train the neural network model using caret with cross-validation
  set.seed(123)
  nn_model_cv <- train(
    PREMIUM_log ~ .,
    data = train_data,
    method = "nnet",
    trControl = train_control,
    tuneGrid = tune_grid,
    linout = TRUE,
    trace = FALSE,
    maxit = 100
  )

  # Save the neural network model with cross-validation as an RDS file
  saveRDS(nn_model_cv, model_path_cv)
  cat("Neural network model with cross-validation saved as", model_path_cv, "\n")
}

nn_model_cv
plot(nn_model_cv)
# Predict on the test data using the best model
test_data$predicted_premium_log_cv <- predict(nn_model_cv, test_data)

# Convert predicted log values back to the original scale
test_data$predicted_premium_cv <- exp(test_data$predicted_premium_log_cv)

# Evaluate the model
mse_cv <- mean((test_data$PREMIUM_log - test_data$predicted_premium_log_cv)^2)

r_squared_cv <- 1 - mse_cv / var(test_data$PREMIUM_log)

rmse_cv <- sqrt(mse_cv)

mae_cv <- mean(abs(test_data$PREMIUM_log - test_data$predicted_premium_log_cv))

mape_cv <- mean(abs(test_data$PREMIUM_log - test_data$predicted_premium_log_cv) / test_data$PREMIUM_log) * 100

# print evaluations of the model as a table
results_cv <- data.frame(Model = "Neural Network Cross Validation",
                         MSE = mse_cv,
                         R_squared = r_squared_cv,
                         RMSE = rmse_cv,
                         MAE = mae_cv,
                         MAPE = mape_cv)

#results_cv

# Visualize Predicted vs. Actual Premium (Original Scale) with CV
ggplot(test_data, aes(x = PREMIUM, y = predicted_premium_cv)) +
  geom_point(color = "darkblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Actual Premium",
       y = "Predicted Premium (CV)",
       title = "Predicted vs. Actual Premium with Cross-Validation") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_text(face = "bold", colour = "blue", size = 12),
        axis.title.y = element_text(face = "bold", colour = "blue", size = 12))
```

## Results

| Model                          | Mean Squared Error (MSE) | R-squared | Root Mean Squared Error (RMSE) | Mean Absolute Error (MAE) | Mean Absolute Percentage Error (MAPE) |
|---------|:------:|:-----:|:----:|:------:|:----:|
| Neural Network                 | 0.2345529                | 0.7510002 | 0.4843067                     | 0.3009332                 | 3.488952 %                            |
| Neural Network Cross Validation| 0.2343864                | 0.751177  | 0.4841347                     | 0.2981682                 | 3.462526 %                            |

The weights decay plot shows how the RMSE (Root Mean Square Error) from cross-validation varies with the number of hidden units in a neural network for different weight decay values (0, 0.01, and 0.1). As the number of hidden units increases from 3 to 5, RMSE decreases across all weight decay values, suggesting improved model accuracy with additional capacity. However, beyond 5 hidden units, RMSE levels off or slightly increases, especially when weight decay is low or absent, indicating potential overfitting. Weight decay, a regularization technique to prevent overfitting, has a noticeable effect as the number of hidden units increases; while it slightly raises RMSE at lower hidden units, it helps to control error at higher hidden units. The optimal configuration, with the lowest RMSE, occurs at 5 hidden units regardless of weight decay, though weight decay of 0.1 becomes more beneficial as the model complexity increases, particularly at 6 and 7 hidden units.

The results from both the neural network and the neural network with cross-validation are very similar, with only minor differences in the evaluation metrics. This consistency suggests that the model is robust and performs well regardless of the validation method used. The cross-validation approach confirms the reliability of the neural network model, indicating that it is not overfitting and generalizes well to unseen data.

The evaluation of the neural network model revealed a Mean Squared Error (MSE) of 0.23, indicating the average squared difference between the actual and predicted log-transformed premium amounts. The R-squared value of 0.75 suggests that the model can explain approximately 75.10% of the variance in the log-transformed premiums, indicating a good fit to the data. The Root Mean Squared Error (RMSE) of 0.48 represents the square root of the MSE, providing a measure of the model's prediction accuracy. The Mean Absolute Error (MAE) of 0.30 indicates the average absolute difference between the actual and predicted log-transformed premiums. The Mean Absolute Percentage Error (MAPE) of 3.46% represents the average percentage difference between the actual and predicted premiums, providing a measure of the model's relative accuracy.

Overall, the neural network model demonstrates good performance in predicting the premium amounts based on the characteristics of the insured vehicles and drivers. The model captures the underlying patterns in the data and provides accurate predictions of the premium amounts. The evaluation metrics indicate that the model has a high level of accuracy and predictive power, which could make it a valuable tool for premium prediction in the insurance industry.


## Support Vector Machine (SVM)
For the scenario of SVM models, it has been decided to do multiple-classifications for the premiums and divide it into 4 levels from low to very high.
```{r SVM MCC, cache=TRUE}
# Setup parallelisation
cl <- makeCluster(detectCores() - 1) 
registerDoParallel(cl)

ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

tune_grid <- expand.grid(
  .C = c(0.1, 1, 10),       
  .sigma = c(0.01, 0.1, 0.5)
)

set.seed(123)
svm_data <- clean_dat_motor[sample(nrow(clean_dat_motor), 1000, replace = FALSE), ] # only 1000 oobservations due to performance.

# Visualize the distribution and density of PREMIUM_log
hist(svm_data$PREMIUM_log, main = "Premium Distribution", xlab = "Premium", breaks = 30)
plot(density(svm_data$PREMIUM_log, na.rm = TRUE), main = "Density of Premium")

# Calculate percentiles and create custom categories for PREMIUM
percentiles <- quantile(svm_data$PREMIUM_log, probs = c(0.25, 0.5, 0.75))
svm_data$premium_category <- cut(svm_data$PREMIUM_log,
                                 breaks = c(-Inf, percentiles[1], percentiles[2], percentiles[3], Inf), 
                                 labels = c("low", "medium", "high", "very_high"))
print(table(svm_data$premium_category))

# Split into train & test data 70/30
set.seed(123)
trainIndex <- createDataPartition(svm_data$premium_category, p = 0.7, list = FALSE)
train_data <- svm_data[trainIndex, ]
test_data <- svm_data[-trainIndex, ]

# Cross validation and hypertuning
svm_model <- train(premium_category ~ SEX + AGE_VEHICLE + INSURED_VALUE_log + CLAIM_PAID_USD_log + 
                     CCM_TON_log + MAKE + USAGE,
                   data = train_data,
                   method = "svmRadial",     # radial kernel as there seems to be no linear relationship
                   trControl = ctrl,         
                   tuneGrid = tune_grid)     
stopCluster(cl)

# Evaluation of best model
print(svm_model$bestTune)     # Best hyper parameters
print(svm_model$finalModel)   # best model

print(svm_model)

# Predictions on training data
pred_train <- predict(svm_model, train_data)
conf_matrix_train <- confusionMatrix(pred_train, train_data$premium_category)
print("Confusion metrics for TEST_DATA")
print(conf_matrix_train)

mcc_train <- mccr(pred_train, train_data$premium_category)
print(paste("MCC for Train Data:", round(mcc_train, 4))) # something seems off here... manual calculation leads to roughly 0.58 whcih incidates a moderate positive correlation between predicted and actual classifications.
# manual calculation

conf_table <- as.table(conf_matrix_train)
TP <- conf_table[2, 2]  # True Positive
TN <- conf_table[1, 1]  # True Negative
FP <- conf_table[1, 2]  # False Positive
FN <- conf_table[2, 1]  # False Negative

# Calculate MCC
mcc <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))

# Print MCC
print(paste("MCC Train manually calculated:", round(mcc, 4)))

# Predictions on test data
pred_test <- predict(svm_model, newdata = test_data)

conf_matrix <- confusionMatrix(pred_test, test_data$premium_category)
print("Confusion metrics for TEST_DATA")
print(conf_matrix)

# Usage of MCC due to imbalanced dataset
mcc_test <- mccr(pred_test, test_data$premium_category)
print(paste("MCC for Test Data:", round(mcc_test, 4))) # something seems off here... manual calculation leads to roughyl 0.7 which is considered strong correlation between predicted and actual classifications.

# manual calculation

conf_table <- as.table(conf_matrix)
TP <- conf_table[2, 2]  # True Positive
TN <- conf_table[1, 1]  # True Negative
FP <- conf_table[1, 2]  # False Positive
FN <- conf_table[2, 1]  # False Negative

# Calculate MCC
mcc <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))

# Print MCC
print(paste("MCC manually calculated:", round(mcc, 4)))

```

This analysis explores the performance of a multiclass classification task using an SVM model with a radial kernel. The goal was to classify PREMIUM_log into categories: "low," "medium," "high," and "very_high," employing hyperparameter tuning and parallel computation for efficiency. Data preparation included sampling 1000 entries from the cleaned dataset and visualizing the distribution of PREMIUM_log, followed by a 70/30 split for training and testing. It is important to note that the dataset is imbalanced, which may pose challenges in model training and evaluation, potentially impacting the reliability of certain performance metrics.

Model training was carried out with 10-fold cross-validation repeated three times, leveraging parallel processing to speed up the evaluation. A grid search was performed to find optimal values for the hyperparameters c (cost) and sigma, ensuring the model's robustness and generalizability.

The evaluation showed that the "very_high" category had the highest sensitivity at 0.8629, while the "low" category excelled in specificity at 0.9695 and positive predictive value at 0.8926. The MCC for each class revealed strong performance overall: ~0.986 for "low," ~0.802 for "medium," ~0.781 for "high," and ~1.038 for "very_high," though the latter may indicate overestimation and warrants further review.

The code process incorporated parallelized cross-validation and grid search, facilitating comprehensive hyperparameter tuning. The findings highlighted an overall accuracy of 0.7771 and a Kappa statistic of 0.7029, with McNemar's test yielding a significant P-value of 0.008264 and mcc-value of roughly 0.57, suggesting a noteworthy difference from random classification.

To improve the model, checking the training set's class distribution and considering resampling techniques like random oversampling or SMOTE could be performed to further improve the model. 

In conclusion, while the model showed strong results for the "low" and "very_high" categories, further optimization is needed for "medium" and "high" to enhance overall performance.

Nevertheless, the model demonstrates reliable performance in classifying insurance premiums into the four categories with MCC of about 0.57 indicating a moderate to strong correlation between prediction and actual category.Therefore, the robust model can be used to classify premiums.Further refinement of tailored features may improve overall performance, especially for medium and high categories.


# Conclusion

# TODO

# TODO remove of do whatever you want

```{r lm_interaktion, cache=TRUE, eval=FALSE}

#Coole Plots zum Aufzeigen von Interaktionen:

# Boxplot von PREMIUM_log nach MAKE und USAGE
ggplot(clean_dat_motor, aes(x=MAKE, y=PREMIUM_log, fill=USAGE)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Interaktion zwischen MAKE und USAGE", x = "MAKE", y = "PREMIUM_log")

# Boxplot von PREMIUM_log nach TYPE_VEHICLE und USAGE
ggplot(clean_dat_motor, aes(x=TYPE_VEHICLE, y=PREMIUM_log, fill=USAGE)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Interaktion zwischen TYPE_VEHICLE und USAGE", x = "TYPE_VEHICLE", y = "PREMIUM_log")

# Scatterplot von INSURED_VALUE_log und PREMIUM_log nach USAGE
ggplot(clean_dat_motor, aes(x=INSURED_VALUE_log, y=PREMIUM_log, color=USAGE)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", se=FALSE) +
  labs(title = "Interaktion zwischen INSURED_VALUE_log und USAGE", x = "INSURED_VALUE_log", y = "PREMIUM_log")

```



### Massnahme 1): Quasi-Poisson-Regression

```{r quasiPois, cache=TRUE, eval=FALSE}

#Quasi-Poisson
quasi_poisson_model <- glm(Amount_Claims ~ SEX + TYPE_VEHICLE + INSR_TYPE + MAKE + AGE_VEHICLE + SEATS_NUM,
                           family = quasipoisson, data = dat_amount_claims)

# Summary of the model
summary(quasi_poisson_model)
#F-Test, Vergleiche von geschachtelten Quasi-Likelihood-Modellen (Overdispersion)
drop1(quasi_poisson_model, test= "F")

# Model diagnostics: Residuals vs. Fitted Plot
plot(quasi_poisson_model$fitted.values, residuals(quasi_poisson_model, type = "deviance"),
     xlab = "Fitted Values",
     ylab = "Deviance Residuals",
     main = "Residuals vs Fitted (Quasi-Poisson)")
abline(h = 0, col = "red")

# QQ plot of residuals (checking normality)
qqnorm(residuals(quasi_poisson_model, type = "deviance"), main = "QQ Plot of Residuals (Quasi-Poisson)")
qqline(residuals(quasi_poisson_model, type = "deviance"), col = "red")



```
Switching to a quasi-Poisson model to account for overdispersion led to improvements compared to the original Poisson model. The residuals vs. fitted plot shows a reduced dispersion of the residuals at higher estimated values, which indicates a better fit of the variance, although heteroscedasticity still exists. The QQ plot of the residuals shows an improved fit to the theoretical normal distribution, especially in the middle range, while deviations at the edges remain, indicating extreme values or modelling errors. By adjusting the dispersion parameter (29.765) in the quasi-Poisson model, the increased variance compared to the Poisson model is adequately taken into account. The F-test confirms the significance of the variables ‘SEX’, ‘TYPE_VEHICLE’, ‘MAKE’, ‘AGE_VEHICLE’ and ‘SEATS_NUM’. Despite these improvements, there are still slight anomalies in the residuals.