---
pdf_document: default
authors: "Alvaro Cervan, Luca Renz, Rafaella Miranda-Sousa"
date: "`r Sys.Date()`"
output:
  #word_document: default
  pdf_document: default
  #html_document: 
  #  theme: darkly
    
    
title: "ML1"
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, cache=TRUE, results='asis', fig.height=4.4)
```

```{css REFINE OR DELETE, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 100px;
}

img{
  width: 100%;
  height: auto;
}
```

```{r load_packages, echo =FALSE, cache=TRUE}

#Install and import libraries.
if (!require("car")) install.packages("car")  # Support Vector Machine (SVM)
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("ROI")) install.packages("ROI")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("lubridate")) install.packages("lubridate")
if (!require("corrplot")) install.packages("corrplot")
if (!require("dplyr")) install.packages("dplyr")
if (!require("GGally")) install.packages("GGally")
if (!require("mgcv")) install.packages("mgcv")  # Generalised Additive Model (GAM)
if (!require("nnet")) install.packages("nnet")  # Neural Networks
if (!require("e1071")) install.packages("e1071")  # Support Vector Machine (SVM)
if (!require("MASS")) install.packages("MASS")
if (!require("arm")) install.packages("arm")
if (!require("caret")) install.packages("caret")
if (!require("pscl")) install.packages("pscl")
if (!require("doParallel")) install.packages("doParallel")
if (!require("rcompanion")) install.packages("rcompanion")
if (!require("gam")) install.packages("gam")
if (!require("MLmetrics")) install.packages("MLmetrics")
if (!require("mccr")) install.packages("mccr")
if (!require("NeuralNetTools")) install.packages("NeuralNetTools")

library(car)
library(readr,quietly = T)
library(ggplot2, quietly = T)
library(ROI, quietly = T)
library(tidyverse, quietly = T)
library(lubridate, quietly = T)
library(corrplot, quietly = T)
library(dplyr)
library(GGally)
library(mgcv)  # For Generalised Additive Models
library(nnet)  # For Neural Networks
library(e1071)  # For Support Vector Machine
library(arm)
library(caret)
library(pscl) #zeroinflated
library(rcompanion)
library(gam)
library(doParallel)#SVM Parallelisierung
library(MLmetrics)  # For MCC calculation
library(mccr)
library(NeuralNetTools) # For Neural Network visualization

```
# Introduction
One of the major challenges for insurances is to estimate the appropriate premiums to charge each customer while not risking to lose any money. Therefore, this project aims at supporting an Ethiopian Insurance company to understand how their customers can benefit from having the most accurate and fair premium as they need and have to pay. Machine Learning helps in this case enormously to understand, what factors have a larger impact on the premium and how customers can be classified accordingly.

In this document, the reader may find different algorithms to solve various aspects of the premium-calculations.

# Data Preprocessing

In order to apply such algorithms, the data has to be pre-processed. 
In a very brief summary, the script removes unnecessary columns and duplicates, and handles missing and zero values, particularly for columns like INSURED_VALUE and SEATS_NUM. It converts certain columns to more meaningful categories, such as transforming SEX into factors representing legal entities and genders. The script also filters data to exclude irrelevant vehicle types and usage, ensuring the final dataset contains only pertinent records. Finally, it summarizes and adjusts the dataset further by converting appropriate columns into factors and removing variables not required for analysis.

```{r data_prep, echo=FALSE, cache=TRUE}

#DATA LOAD
raw_dat_motor <- read_csv("data/motor_data14-2018.csv", show_col_types = FALSE)

#dim(raw_dat_motor)
#str(raw_dat_motor)

#DATA PREP

#EFFECTIVE_YR
#Remove the column EFFECTIVE_YR (Not useful as it cannot be deciphered)
raw_dat_motor <- raw_dat_motor[ , !(names(raw_dat_motor) %in% "EFFECTIVE_YR")]

#CARRYING_CAPACITY
#Remove the column CARRYING_CAPACITY
raw_dat_motor <- raw_dat_motor[ , !(names(raw_dat_motor) %in% "CARRYING_CAPACITY")]

#CLAIM_PAID
raw_dat_motor$CLAIM_PAID_USD <- ifelse(is.na(raw_dat_motor$CLAIM_PAID), 0, raw_dat_motor$CLAIM_PAID)
raw_dat_motor$CLAIM_PAID <- ifelse(raw_dat_motor$CLAIM_PAID_USD == 0, "NO", "YES")

#Remove duplicates and count the number of removed rows
removed_count <- nrow(raw_dat_motor) - nrow(raw_dat_motor <- distinct(raw_dat_motor))
#Output the number of removed duplicates
# cat("Number of removed duplicates:", removed_count, "\n")

#SEX
raw_dat_motor$SEX <- factor(raw_dat_motor$SEX, 
                            levels = c(0, 1, 2), 
                            labels = c("Legal entity", "Male", "Female"))
#table(raw_dat_motor$SEX)

#INSR_BEGIN
raw_dat_motor$INSR_BEGIN <- dmy(raw_dat_motor$INSR_BEGIN)

#INSR_END
raw_dat_motor$INSR_END <- dmy(raw_dat_motor$INSR_END)

#DURATION
raw_dat_motor$DURATION <- as.numeric(as.Date(raw_dat_motor$INSR_END) - as.Date(raw_dat_motor$INSR_BEGIN))

hist(raw_dat_motor$DURATION)

#Gleiche DURATION (Vertragsdauer) zur Vergleichbarkeit
raw_dat_motor <- raw_dat_motor[raw_dat_motor$DURATION == 364, ]

#Extract year of insurance start and add as a new variable
raw_dat_motor$START_INS_YR <- year(as.Date(raw_dat_motor$INSR_BEGIN))

#INSR_TYPE
raw_dat_motor$INSR_TYPE <- factor(raw_dat_motor$INSR_TYPE, 
                                  levels = c(1201, 1202, 1204), 
                                  labels = c("Private", "Commercial", "Motor trade road risk"))

#INSURED_VALUE
#Check the number of missing values
missing_values <- sum(is.na(raw_dat_motor$INSURED_VALUE))
#cat("Missing values in INSURED_VALUE:", missing_values, "\n")

#Summary of the statistical key figures
summary_stats <- summary(raw_dat_motor$INSURED_VALUE)
#cat("Summary of the statistical key figures of INSURED_VALUE:\n")
#print(summary_stats)

#Determine the number of entries with a value of 0
zero_values <- sum(raw_dat_motor$INSURED_VALUE == 0, na.rm = TRUE)
#cat("Number of entries with the value 0 in INSURED_VALUE:", zero_values, "\n")

#Check how many records are affected
zero_insured_value <- raw_dat_motor[raw_dat_motor$INSURED_VALUE == 0, ]
#cat("Number of data records with INSURED_VALUE = 0:", nrow(zero_insured_value), "\n")

#Summary of affected records by various variables to identify patterns
#cat("Distribution of INSR_TYPE with INSURED_VALUE = 0:\n")
#table(raw_dat_motor$INSR_TYPE) #All
#print(table(zero_insured_value$INSR_TYPE)) #Only 0

#cat("\nDistribution of TYPE_VEHICLE with INSURED_VALUE = 0:\n")
#print(table(zero_insured_value$TYPE_VEHICLE))

#cat("\nDistribution of USAGE with INSURED_VALUE = 0:\n")
#print(table(zero_insured_value$USAGE))

#Statistical key figures for other variables with INSURED_VALUE = 0 (e.g., PREMIUM)
#cat("\nSummary of the PREMIUM with INSURED_VALUE = 0:\n")
#summary(zero_insured_value$PREMIUM)

#Visualization of vehicle usage with INSURED_VALUE = 0
ggplot(zero_insured_value, aes(x = USAGE)) +
  geom_bar(fill = "blue", color = "black") +
  labs(title = "Distribution of USAGE with INSURED_VALUE = 0", x = "USAGE", y = "Amount") +
  theme_minimal()

#Conclusion: No connection was found. Presumably, the insurance value of 0 is due to the fact that only third-party liability insurance is legally required. In these cases, there is no fixed value for damage to the vehicle itself. Therefore, records with an INSURED_VALUE of 0 are removed from the analysis as they do not contain any relevant information for the assessment of vehicle values.

#Remove rows where INSURED_VALUE is equal to 0
raw_dat_motor <- raw_dat_motor[raw_dat_motor$INSURED_VALUE != 0, ]

#Verify if the rows have been successfully removed
#cat("Number of remaining data records:", nrow(raw_dat_motor), "\n")

#Visualize the distribution of INSURED_VALUE (Histogram)
ggplot(raw_dat_motor, aes(x = INSURED_VALUE)) +
  geom_histogram(binwidth = 50000, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of INSURED_VALUE", x = "INSURED_VALUE", y = "Frequency") +
  theme_minimal()


#Boxplot to identify outliers
ggplot(raw_dat_motor, aes(y = INSURED_VALUE)) +
  geom_boxplot(fill = "orange", color = "black", alpha = 0.7) +
  labs(title = "Boxplot of INSURED_VALUE", y = "INSURED_VALUE") +
  theme_minimal()


#Check the statistical key figures without 0 values
#cat("Summary of the statistical key figures without 0 values:\n")
#print(summary(raw_dat_motor$INSURED_VALUE))

#Distribution of the log-transformed INSURED_VALUE (only for non-zero values)
ggplot(raw_dat_motor[raw_dat_motor$INSURED_VALUE > 0, ], aes(x = log(INSURED_VALUE))) +
  geom_histogram(binwidth = 0.2, fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Log-transformed distribution of INSURED_VALUE (without zero values)", x = "log(INSURED_VALUE)", y = "Frequency") +
  theme_minimal()

#summary(raw_dat_motor$INSURED_VALUE)

#Remove outliers in INSURED_VALUE
raw_dat_motor <- raw_dat_motor[raw_dat_motor$INSURED_VALUE >= 10, ]
#summary(raw_dat_motor$INSURED_VALUE)

#Workspace cleanup
rm(zero_insured_value)

#PREMIUM
#Check if removing PREMIUM = 0 from the dataset is valid
data.frame(
  PREMIUM_0_Percent = round(100 * sum(raw_dat_motor$PREMIUM == 0, na.rm = TRUE) / sum(!is.na(raw_dat_motor$PREMIUM)), 4),
  PREMIUM_NA_Percent = round(100 * sum(is.na(raw_dat_motor$PREMIUM)) / sum(!is.na(raw_dat_motor$PREMIUM)), 4),
  PREMIUM_MORE_Percent = round(100 * sum(raw_dat_motor$PREMIUM > 0, na.rm = TRUE) / sum(!is.na(raw_dat_motor$PREMIUM)), 4)
)

#Remove rows where PREMIUM is NA or 0
raw_dat_motor <- raw_dat_motor[!(is.na(raw_dat_motor$PREMIUM) | raw_dat_motor$PREMIUM == 0), ]

# Remove duplicates and count the number of removed rows
removed_count <- nrow(raw_dat_motor) - nrow(raw_dat_motor <- distinct(raw_dat_motor))
# Output the number of removed duplicates
cat("Number of removed duplicates:", removed_count, "\n")

# OBJECT_ID
# Total number of rows in the dataset
total_rows <- nrow(raw_dat_motor)

# Number of unique OBJECT_IDs
unique_object_ids <- length(unique(raw_dat_motor$OBJECT_ID))

# Check if OBJECT_IDs are unique
if (total_rows == unique_object_ids) {
  cat("The OBJECT_IDs are unique.\n")
} else {
  cat("The OBJECT_IDs are NOT unique.\n")
  cat("Number of duplicates:", total_rows - unique_object_ids, "\n")
  
  # Frequency of OBJECT_IDs
  object_id_counts <- table(raw_dat_motor$OBJECT_ID)
  
  # Average and maximum frequency of OBJECT_ID
  avg_object_id_freq <- mean(object_id_counts)
  max_object_id_freq <- max(object_id_counts)
  
  cat("Average frequency of the OBJECT_ID:", round(avg_object_id_freq, 3), "\n")
  cat("Maximum frequency of the OBJECT_ID:", max_object_id_freq, "\n")
  
  # Frequency of the combination of OBJECT_ID, INSR_BEGIN, INSR_END, INSURED_VALUE, and PREMIUM
  combo_counts <- raw_dat_motor %>%
    group_by(OBJECT_ID, INSR_BEGIN, INSR_END, INSURED_VALUE, PREMIUM) %>%
    summarise(count = n(), .groups = 'drop')
  
  # Average and maximum frequency of the combination
  avg_combo_freq <- mean(combo_counts$count) # Average frequency of the combination
  max_combo_freq <- max(combo_counts$count)  # Maximum frequency of the combination
  
  cat("Average frequency of the combination (OBJECT_ID, INSR_BEGIN, INSR_END, INSURED_VALUE, PREMIUM):", round(avg_combo_freq, 3), "\n")
  cat("Maximum frequency of the combination (OBJECT_ID, INSR_BEGIN, INSR_END, INSURED_VALUE, PREMIUM):", max_combo_freq, "\n")
}

#In some cases, CLAIM_PAID == YES results in duplication
#Correction or removal of these duplicate rows
raw_dat_motor <- raw_dat_motor %>%
  group_by(SEX, INSR_BEGIN, INSR_END, INSR_TYPE, INSURED_VALUE, PREMIUM, OBJECT_ID, PROD_YEAR, SEATS_NUM, TYPE_VEHICLE, CCM_TON, MAKE, USAGE) %>%
  # Count the number of rows in each group
  mutate(group_size = n()) %>%
  ungroup() %>%
  # Remove rows only if it's a duplicate and CLAIM_PAID == "NO" and CLAIM_PAID_USD <= 1
  filter(!(group_size > 1 & CLAIM_PAID == "NO" & CLAIM_PAID_USD <= 1)) %>%
  dplyr::select(-group_size)  # Remove the helper column

#Correction of inconsistency in INSR_TYPE
raw_dat_motor <- raw_dat_motor %>%
  group_by(SEX, INSR_BEGIN, INSR_END, INSURED_VALUE, PREMIUM, OBJECT_ID, PROD_YEAR, SEATS_NUM, TYPE_VEHICLE, CCM_TON, MAKE, USAGE) %>%
  filter(!(n() > 1 & INSR_TYPE != "Commercial")) %>%  # Keep only rows with "Commercial"
  ungroup()  # Ungroup to remove the grouping object

# Clear Workspace
rm(list = setdiff(ls(), "raw_dat_motor"))

#PROD_YEAR
#Remove rows with PROD_YEAR as NA
raw_dat_motor <- raw_dat_motor[!is.na(raw_dat_motor$PROD_YEAR),]
summary(raw_dat_motor$PROD_YEAR)

#SEATS_NUM
#Analysis of SEATS_NUM: Number of rows with SEATS_NUM == 0, NA, and other values
data.frame(
  SEATS_NUM_0 = sum(raw_dat_motor$SEATS_NUM == 0, na.rm = TRUE),
  SEATS_NUM_NA = sum(is.na(raw_dat_motor$SEATS_NUM)),
  SEATS_NUM_OTHER = sum(raw_dat_motor$SEATS_NUM > 0, na.rm = TRUE)
)

#Relative: Percentage of rows with SEATS_NUM == 0, NA, or other values
data.frame(
  SEATS_NUM_0_or_NA_Percent = 100 * sum(is.na(raw_dat_motor$SEATS_NUM) | raw_dat_motor$SEATS_NUM == 0) / nrow(raw_dat_motor),
  SEATS_NUM_OTHER_Percent = 100 * sum(raw_dat_motor$SEATS_NUM > 0, na.rm = TRUE) / nrow(raw_dat_motor)
)

#Remove rows with SEATS_NUM as NA
raw_dat_motor <- raw_dat_motor[!is.na(raw_dat_motor$SEATS_NUM),]
summary(raw_dat_motor$SEATS_NUM)

#Issue: There are SEATS_NUM values equal to 0

##Create separate datasets for SEATS_NUM == 0 and SEATS_NUM > 0
#data_seats_num_0 <- subset(raw_dat_motor, SEATS_NUM == 0)
#data_seats_num_other <- subset(raw_dat_motor, SEATS_NUM > 0 & !is.na(SEATS_NUM))
## Tables of vehicle types for both datasets
#table(data_seats_num_0$TYPE_VEHICLE)
#table(data_seats_num_other$TYPE_VEHICLE)

##SEATS_NUM Alternative 1: Remove rows where SEATS_NUM is 0 or NULL
##List of vehicle types where SEATS_NUM == 0 is implausible (Trailers and semitrailers, Tractor, Tanker are retained)
#unplausible_types <- c("Automobile", "Bus", "Motor-cycle", "Pick-up", "Station Wagones", "Tanker", "Truck")
##Remove rows where SEATS_NUM == 0 and the vehicle type is implausible
#raw_dat_motor <- raw_dat_motor[!(raw_dat_motor$SEATS_NUM == 0 & raw_dat_motor$TYPE_VEHICLE %in% unplausible_types), ]
#
#
##SEATS_NUM Alternative 2: Remove rows where SEATS_NUM is 0 or NULL
#raw_dat_motor <- raw_dat_motor[!(is.na(raw_dat_motor$SEATS_NUM) | raw_dat_motor$SEATS_NUM == 0), ]
#
##SEATS_NUM Alternative 3: Remove the SEATS_NUM column due to many NAs and poor data quality
##raw_dat_motor <- subset(raw_dat_motor, select = -SEATS_NUM)

#TYPE_VEHICLE
#Remove rows where TYPE_VEHICLE == "Trade plates" due to insufficient occurrences (5)
#table(raw_dat_motor$TYPE_VEHICLE)

raw_dat_motor <- raw_dat_motor[raw_dat_motor$TYPE_VEHICLE != "Trade plates", ]

#CCM_TON
#summary(raw_dat_motor$CCM_TON)
#Relative CCM_TON 0
data.frame(
  CCM_TON_0_Percent = 100 * mean(raw_dat_motor$CCM_TON == 0, na.rm = TRUE),
  CCM_TON_MORE_Percent = 100 * mean(raw_dat_motor$CCM_TON > 0, na.rm = TRUE))
# Create separate datasets for CCM_TON == 0 and CCM_TON > 0
data_CCM_TON_0 <- subset(raw_dat_motor, CCM_TON == 0)
data_CCM_TON_other <- subset(raw_dat_motor, CCM_TON > 0 & !is.na(CCM_TON))
# Tables of vehicle types for both datasets
#table(data_CCM_TON_0$TYPE_VEHICLE)
#table(data_CCM_TON_other$TYPE_VEHICLE)
# List of vehicle types where CCM_TON == 0 is implausible (Tractor, Trailers, and semitrailers are retained)
unplausible_types_ccm <- c("Automobile", "Bus", "Motor-cycle", "Pick-up", "Truck", "Station Wagones", "Tanker", "Special construction")
# Remove rows where CCM_TON == 0 and the vehicle type is implausible
raw_dat_motor <- raw_dat_motor[!(raw_dat_motor$CCM_TON == 0 & raw_dat_motor$TYPE_VEHICLE %in% unplausible_types_ccm), ]


#USAGE
#table(raw_dat_motor$USAGE)
#Remove rows with few occurrences
raw_dat_motor <- subset(raw_dat_motor, !(USAGE %in% c("Fire fighting", "Learnes", "Others")))



#MAKE
#Remove rows where MAKE is NA
raw_dat_motor <- raw_dat_motor[!is.na(raw_dat_motor$MAKE),]
#Convert MAKE to uppercase
raw_dat_motor$MAKE <- toupper(raw_dat_motor$MAKE)

# Remove duplicates and count the number of removed rows
removed_count <- nrow(raw_dat_motor) - nrow(raw_dat_motor <- distinct(raw_dat_motor))
# Output the number of removed duplicates
#cat("Number of removed duplicates:", removed_count, "\n")
# Keep rows where MAKE starts with a letter
raw_dat_motor <- raw_dat_motor[grepl("^[A-Za-z]", raw_dat_motor$MAKE), ]
#table(raw_dat_motor$MAKE)



#Manual corrections from MAKE
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("PORCHE", "FORSCHE")] <- "PORSCHE"
raw_dat_motor$MAKE[raw_dat_motor$MAKE == "YAMHA"] <- "YAMAHA"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("VOLKSWAGON", "VOLKS WAGON")] <- "VOLKSWAGEN"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("TOYOTAA", "TOYOTA*", "TOYTA", "TOYOTA AUTOMOBILE",
                                             "TOYATA", "T0Y0TA", "COMPACT YARIS", "YARIS",
                                             "LAND CRUISER", "VITZ")] <- "TOYOTA"
raw_dat_motor$MAKE[grepl("^TOYOTA", raw_dat_motor$MAKE)] <- "TOYOTA"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("NISAN", "NISSAN*")] <- "NISSAN"
raw_dat_motor$MAKE[grepl("^NISSAN", raw_dat_motor$MAKE)] <- "NISSAN"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("RENGE  ROVER", "RANGEROVER")] <- "RANGE ROVER"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("RENALT", "RENUALT", "RENAULT/STOLARCZYK", "RENAULT*")] <- "RENAULT"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("PEUGEOUT", "PEJOT", "PAGOT")] <- "PEUGEOT"
raw_dat_motor$MAKE[grepl("^PEUGEOT", raw_dat_motor$MAKE)] <- "PEUGEOT"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("BMB")] <- "BMW"
raw_dat_motor$MAKE[grepl("^BMW", raw_dat_motor$MAKE)] <- "BMW"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("MISTIBUSH", "MITSUBISHI*", "MITSUBUSHI")] <- "MITSUBISHI"
raw_dat_motor$MAKE[grepl("^FORD", raw_dat_motor$MAKE)] <- "FORD"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("SPORTAGE")] <- "KIA"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("MERCEEDES", "MERCEEDICE", "MERCEDICE", "MERCHEDES")] <- "MERCEDES"
raw_dat_motor$MAKE[grepl("^MERCEDES", raw_dat_motor$MAKE)] <- "MERCEDES"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("SUZIKE")] <- "SUZUKI"
raw_dat_motor$MAKE[grepl("^SUZUKI", raw_dat_motor$MAKE)] <- "SUZUKI"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("HYUNDI GETZ")] <- "HYUNDAI"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("ISUSU")] <- "ISUZU"
raw_dat_motor$MAKE[grepl("^ISUZU", raw_dat_motor$MAKE)] <- "ISUZU"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("FRANKUN IVECO")] <- "IVECO"
raw_dat_motor$MAKE[grepl("^IVECO", raw_dat_motor$MAKE)] <- "IVECO"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("LANDROVER")] <- "LAND ROVER"


raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("DONGFANG", "DONFING", "DONG FENGSHEN", "DONG FENG")] <- "DONGFENG"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("MAHANDRA")] <- "MAHINDRA"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("HOLAND CAR")] <- "ABAY"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("CALABRASE")] <- "CALABRESE"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("GELYION", "GENLION", "GELION", "GENLYONIVECO", "HONGYAN")] <- "GENLYON"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("DHATSU", "DIATSU", "DIAHATSU")] <- "DAIHATSU"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("DAWOO", "DAWWO", "DEAWOO", "DEAWOO USE")] <- "DAEWOO"
raw_dat_motor$MAKE[grepl("^LIFAN", raw_dat_motor$MAKE)] <- "LIFAN"
raw_dat_motor$MAKE[grepl("^BAIC", raw_dat_motor$MAKE)] <- "BAIC"
raw_dat_motor$MAKE[grepl("^LOADER", raw_dat_motor$MAKE)] <- "LOADER"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("MOTOR CYCLE", "MOTOR  CYCLE")] <- "MOTORCYCLE"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("AUTO", "AUTOMOBIL")] <- "AUTOMOBILE"
raw_dat_motor$MAKE[grepl("^CATERPILLAR", raw_dat_motor$MAKE)] <- "CATERPILLAR"
raw_dat_motor$MAKE[raw_dat_motor$MAKE %in% c("LIBER DOZER", "LIBERR MOBILE CRANE",
                                             "LIEBERR MOBILE CRANE", "LEABER CATO CRANE",
                                             "LEBHER CRANE", "CRANE LEBEHER",
                                             "CRANE LEBHERER", "LIBER", "LIBEHER CRANE",
                                             "LIBERR MOBILECRANE", "CRANELEBHER")] <- "LIEBHERR"

#table(raw_dat_motor$MAKE)

#Count the number of entries per MAKE where CLAIM_PAID == ‘YES’
count_claims_paid <- raw_dat_motor %>%
  group_by(MAKE, CLAIM_PAID ) %>%                 #Group by MAKE
  summarise(count = n()) %>%         #Count the entries per group
  arrange(desc(count)) 

###############################################################################################


#List of preferred vehicle manufacturers (Make)
selected_makes <- c("TOYOTA", "ISUZU", "NISSAN", "IVECO", "SINO HOWO", 
                    "MITSUBISHI", "BISHOFTU", "LIFAN", "FORD", "HYUNDAI", 
                    "MAZDA", "GEELY", "DAEWOO", "MERCEDES", "TATA", 
                    "FIAT", "SINO", "SUZUKI", "GENLYON", "RENAULT")

#Filter the data set according to the selected vehicle manufacturers
clean_dat_motor <- subset(raw_dat_motor, MAKE %in% selected_makes)

#Remove few features
#table(clean_dat_motor$SEX) #OK

#table(clean_dat_motor$USAGE)
clean_dat_motor <- subset(clean_dat_motor, !(USAGE %in% c("Agricultural Any Farm",
                                                          "Agricultural Own Farm",
                                                          "Special Construction", "Taxi")))

#table(clean_dat_motor$TYPE_VEHICLE)
#Remove few features
clean_dat_motor <- subset(clean_dat_motor, !(TYPE_VEHICLE %in% c("Tractor")))

#table(clean_dat_motor$INSR_TYPE)
#Remove few features
clean_dat_motor <- subset(clean_dat_motor, !(INSR_TYPE %in% c("Motor trade road risk")))

#table(clean_dat_motor$MAKE)

#CLAIM_PAID
data.frame(
  CLAIM_PAID_0 = sum(clean_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0 = sum(clean_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))

data.frame(
  CLAIM_PAID_0_Percent = 100 * mean(clean_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0_Percent = 100 * mean(clean_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))

#New Variable: age_vehicle
clean_dat_motor$AGE_VEHICLE <- 2018 - clean_dat_motor$PROD_YEAR
#Remove PROD_YEAR
clean_dat_motor<- clean_dat_motor[,-8]



#New variable: AMOUNT_CLAIMS_PAID (Number of previous claims per object ID)
clean_dat_motor <- clean_dat_motor %>%
  # Für jede object_ID und jedes Startjahr, kumuliere die Anzahl vorheriger Claims mit "YES"
  group_by(OBJECT_ID) %>%
  arrange(OBJECT_ID, START_INS_YR) %>%
  mutate(
    AMOUNT_CLAIMS_PAID = sapply(seq_along(START_INS_YR), function(i) {
      sum(CLAIM_PAID[1:(i-1)] == "YES" & START_INS_YR[1:(i-1)] < START_INS_YR[i])
    })
  ) %>%
  ungroup()


###############################################################################################

#Comparison with the raw data set
#CLAIM_PAID
data.frame(
  CLAIM_PAID_0 = sum(raw_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0 = sum(raw_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))

data.frame(
  CLAIM_PAID_0_Percent = 100 * mean(raw_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0_Percent = 100 * mean(raw_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))

data.frame(
  CLAIM_PAID_0_Percent = 100 * mean(clean_dat_motor$CLAIM_PAID_USD == 0, na.rm = TRUE),
  CLAIM_PAID_MORE_THAN_0_Percent = 100 * mean(clean_dat_motor$CLAIM_PAID_USD > 0, na.rm = TRUE))


###############################################################################################

#NA
colSums(is.na(raw_dat_motor)) 
colSums(is.na(clean_dat_motor)) 

#Remove the variables ‘start of insurance’ and ‘end of insurance’
clean_dat_motor <- subset(clean_dat_motor, select = -c(INSR_BEGIN, INSR_END, DURATION))

#Rearrange the columns: First OBJECT_ID, then START_INS_YR, then the rest
clean_dat_motor <- clean_dat_motor %>%
  dplyr::select(OBJECT_ID, START_INS_YR, SEX, INSR_TYPE, USAGE, TYPE_VEHICLE,
                MAKE, AGE_VEHICLE, SEATS_NUM, CCM_TON, INSURED_VALUE, PREMIUM, CLAIM_PAID_USD, everything())

#Data set too large: Draw 100000 random samples
clean_dat_motor_origin<- clean_dat_motor
set.seed(123)
clean_dat_motor <- sample_n(clean_dat_motor, size = 100000)


#Transformation of categorical variables into factor variables
clean_dat_motor$OBJECT_ID <- as.factor(clean_dat_motor$OBJECT_ID)
clean_dat_motor$SEX <- as.factor(clean_dat_motor$SEX)
clean_dat_motor$INSR_TYPE <- as.character(clean_dat_motor$INSR_TYPE)
clean_dat_motor$INSR_TYPE <- as.factor(clean_dat_motor$INSR_TYPE)
clean_dat_motor$MAKE <- as.factor(clean_dat_motor$MAKE)
clean_dat_motor$USAGE <- as.factor(clean_dat_motor$USAGE)
clean_dat_motor$CLAIM_PAID <- as.factor(clean_dat_motor$CLAIM_PAID)
clean_dat_motor$TYPE_VEHICLE <- as.factor(clean_dat_motor$TYPE_VEHICLE)
clean_dat_motor$START_INS_YR <- as.factor(clean_dat_motor$START_INS_YR)


#Clear Workspace
rm(clean_dat_motor_origin)


```

# Graphical Data Analysis

First, the distribution of the individual numerical variables was analysed to determine whether any transformations were necessary.

```{r graph_data_analysis, cache = TRUE}
# Selection of numeric variables only
numeric_vars <- clean_dat_motor %>%
  select_if(is.numeric)

# Create histograms for each numeric variable
for (var in names(numeric_vars)) {
  print(
    ggplot(clean_dat_motor, aes_string(x = var)) +
      geom_histogram(bins = 30, color = "black", fill = "skyblue") +
      ggtitle(paste("Histogram of", var)) +
      theme_minimal() +
      xlab(var) +
      ylab("Frequency") +
      theme(plot.title = element_text(hjust = 0.5))
  )
}

###################################################################################

#Additional graphical analyses
boxplot(PREMIUM ~ SEX, data = clean_dat_motor,
main = "Premium against sex",
ylab = "Premium")

boxplot(CLAIM_PAID_USD ~ SEX, data = clean_dat_motor,
main = "Claim paid (USD) against sex",
ylab = "Claim paid (USD)")


##############################################

#Log-Transformation
clean_dat_motor$INSURED_VALUE_log <- log(clean_dat_motor$INSURED_VALUE)
clean_dat_motor$PREMIUM_log <- log(clean_dat_motor$PREMIUM)
#Log-transformation using log1p() due to the many 0 values
clean_dat_motor$CLAIM_PAID_USD_log <- log1p(clean_dat_motor$CLAIM_PAID_USD)
clean_dat_motor$CCM_TON_log <- log1p(clean_dat_motor$CCM_TON)


# Clear Workspace
rm(list = setdiff(ls(), c("clean_dat_motor_origin", "clean_dat_motor")))
```

The histograms show that the variables INSURED_VALUE, PREMIUM, CLAIM_PAID_USD and CCM_TON are right-skewed and require a log transformation. The transformed variables will be inserted in the later regression models instead of the original variables.


# Models
Once the pre-processing was completed and a good overview about the given data and domain knowledge what acquired, the team focused on defining models using several different methods shown in followed sub-sections.

## Linear model

A linear model is adapted, whereby CLAIM_PAID_USD_log was not included, as the premium is incurred at the start of the contract and this would therefore not make technical sense. Instead, a bonus-malus system is taken into account by adding AMOUNT_CLAIMS_PAID.

```{r linear_model, cache = TRUE}

#Fit the linear model
lm_model <- lm(PREMIUM_log ~ SEX + INSR_TYPE + USAGE + TYPE_VEHICLE + MAKE +
                 AGE_VEHICLE + SEATS_NUM + CCM_TON_log +INSURED_VALUE_log +
                 AMOUNT_CLAIMS_PAID, data = clean_dat_motor)

#Summary of the model
#summary(lm_model)
#Perform an F-test to drop non-significant categorical variables one at a time
#drop1(lm_model, test= "F") #Bei mehrere Kat Var.
#coefficients
#coef(lm_model)

#Variance Inflation Factor (VIF) for each predictor to check for multicollinearity
#vif(lm_model)

#Residual Analysis
#Q-Q Plot to assess if residuals follow a normal distribution
qqnorm(lm_model$residuals)
qqline(lm_model$residuals, col = "red")

#Residuals vs Fitted Values (check for patterns or heteroscedasticity)
plot(lm_model$fitted.values, lm_model$residuals, 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted Values",
     ylab = "Residuals")
abline(h = 0, col = "red")

#Model Performance
#Calculation of Mean Squared Error (MSE)
mse <- mean(lm_model$residuals^2)
cat("Mean Squared Error (MSE):", mse, "\n")

#Calculation of R-squared value
r_squared <- summary(lm_model)$r.squared
cat("R-squared:", r_squared, "\n")

```


The model summary shows that the Multiple R-squared value is 0.7308, indicating that the model can explain approximately 73.08% of the variance in premiums. This suggests that the model provides a good fit to the data. The F-test for the overall model is significant (p < 2.2e-16), indicating that the predictors as a group have a substantial effect on the premium.

All predictors have a significant impact on the target variable PREMIUM_log. For instance, the categories SEX and USAGE (usage) have a significant effect on PREMIUM_log. Men pay slightly less compared to women, while certain usages, such as "Fare Paying Passengers," lead to higher premiums. In contrast, usages like "Own Goods" and "Private" are associated with lower premiums.

The coefficient of INSURED_VALUE_log (0.7682) in the model shows that the insured value of the vehicle has a strong influence on the premium level. Since both the insured value and the premium are logarithmically transformed, this means that a 1% increase in the insured value results in approximately a 0.7682% increase in the premium. This illustrates the direct and positive relationship between vehicle value and premium: higher-insured vehicles attract proportionally higher premiums, as they represent a greater financial risk for the insurer. Overall, this coefficient confirms that vehicle value is one of the most significant factors in premium calculation.

The coefficient of AMOUNT_CLAIMS_PAID, with a value of 0.1363, indicates that an increase in the number of claims leads to an increase in the log-transformed premium by approximately 0.1363. This means that each additional claim results in a proportional increase in the premium by about 13.63%. This coefficient highlights that an insured’s claim history has a significant impact on the premium level.

The coefficient of AGE_VEHICLE is 0.0029, indicating that with each additional year of vehicle age, the log-transformed premium increases by about 0.0029. Since the target variable is logarithmic, this implies that an additional year in vehicle age leads to a minimal increase in the premium of approximately 0.29%.

The coefficient of SEATS_NUM is -0.00175, which means that with each additional seat, the log-transformed premium decreases by approximately 0.00175. Given the logarithmic nature of the target variable, this can be interpreted as each additional seat leading to a slight reduction in the premium by around 0.175%.

The variable CCM_TON_log has a positive coefficient of 0.0109, indicating a small but statistically significant (p = 0.01241) relationship with the log-transformed premium (PREMIUM_log). This suggests that an increase in the log-transformed vehicle capacity (CCM_TON) is associated with a slight increase in the premium. However, its relatively small effect size indicates it plays a minor role compared to stronger predictors like AMOUNT_CLAIMS_PAID or INSURED_VALUE_log.


VIF:
An analysis of multicollinearity revealed that the Variance Inflation Factor (VIF) for the variable INSR_TYPE is 5.85, which suggests possible multicollinearity. This could affect the model’s stability and interpretability and should be considered in further model optimization.


Residuals Analysis
Residuals vs. Fitted Plot:
The Residuals vs. Fitted Plot displays a funnel-shaped pattern, indicating heteroskedasticity. The variance of the residuals increases with higher predicted values, meaning that the model is less accurate for larger premium values. This violates the assumption of constant variance, suggesting that homoskedasticity is not fully met.

Normal Q-Q Plot:
The Normal Q-Q Plot shows that the residuals do not lie perfectly along the line, indicating significant deviations from the theoretical normal distribution, particularly at the tails. These "heavy tails" suggest a non-normal distribution of residuals, potentially due to outliers or unmodeled non-linear relationships.

To improve the model, various measures could be considered. One approach would be to transform the target variable, for example, using a Box-Cox transformation, to reduce heteroskedasticity and achieve a more stable residual variance. Additionally, incorporating non-linear relationships by including polynomial terms or using a generalized linear model (GLM) could be beneficial. This would allow the model to better capture complex relationships between variables, thereby enhancing predictive accuracy.


## Poisson model

A Poisson model is fitted to predict the number of claims over a 5-year period based on the characteristics SEX, INSR_TYPE, USAGE, TYPE_VEHICLE, MAKE, AGE_VEHICLE, SEATS_NUM, CCM_TON, INSURED_VALUE, and PREMIUM.

First, the data is grouped accordingly, and the results are analyzed to gather insights.

```{r data_for_poisson, cache=TRUE}

#Data preparation: Aggregate the number of claims per combination
dat_amount_claims <- clean_dat_motor %>%
  group_by(SEX, INSR_TYPE, USAGE, TYPE_VEHICLE, MAKE, AGE_VEHICLE, SEATS_NUM, CCM_TON, INSURED_VALUE, PREMIUM) %>%
  summarise(AMOUNT_CLAIMS = sum(CLAIM_PAID == "YES"), .groups = 'drop')

#summary(dat_amount_claims$AMOUNT_CLAIMS)
hist(dat_amount_claims$AMOUNT_CLAIMS, breaks=max(dat_amount_claims$AMOUNT_CLAIMS)) #Zero-inflated Poisson Regression would fit better

#Mean vs. Variance
#mean(dat_amount_claims$AMOUNT_CLAIMS)
#var(dat_amount_claims$AMOUNT_CLAIMS)
#The variance is not much greater than the mean, over-dispersion can be neglegted in the model.

```
The analysis of the distribution of the target variable AMOUNT_CLAIMS reveals that a large portion of the values are zero. This concentration of zero values is confirmed by the median, as well as the 1st and 3rd quartiles, which are also at zero. Additionally, the distribution shows some high outliers with a maximum value of 46, indicating an uneven distribution with a few high values. The low mean of 0.1791 further supports this observation, suggesting a significant number of zero values. Given these distribution characteristics, the use of a Zero-Inflated Poisson (ZIP) model could be appropriate, as such a model can account for both random and structural zeros. Initially, however, a Poisson model will be fitted.

```{r Poisson_Model, cache=TRUE}

#Poisson Regression:
poisson_model <- glm(AMOUNT_CLAIMS ~ SEX + INSR_TYPE + TYPE_VEHICLE + MAKE +
                      AGE_VEHICLE + SEATS_NUM + CCM_TON + INSURED_VALUE + PREMIUM,
                      family = poisson(link = "log"), data = dat_amount_claims)

#Summary of the model
#summary(poisson_model)

#Model coefficients and exponentiated coefficients (Rate Ratios)
#coef(poisson_model)
#exp(coef(poisson_model))

#Diagnose Overdispersion
overdispersion <- deviance(poisson_model) / df.residual(poisson_model)
# Interpretation:
# - A value close to 1 indicates an adequate fit without overdispersion.
# - Values significantly greater than 1 suggest overdispersion, indicating that data variance exceeds the model's assumptions.
#   If overdispersion is present, consider using a Negative Binomial model for a better fit.
#cat("Overdispersion ratio (Deviance / DF):", overdispersion, "\n")

# Poisson Deviance Test: Check Goodness-of-Fit of the model
# Calculate p-value for H0: The model adequately describes the data
p_value_fit <- 1 - pchisq(deviance(poisson_model), df.residual(poisson_model))
#cat("Goodness-of-Fit p-value:", p_value_fit, "\n")
# Interpretation:
# - A high p-value (> 0.05) suggests a good model fit to the data, as we do not reject H0.
# - A low p-value (≤ 0.05) would indicate a poor fit, suggesting that the model does not describe the data well.

# Calculate VIF values for all predictor variables in the model
vif_values <- vif(poisson_model)

# Output the VIF values for interpretation
#cat("Variance Inflation Factor (VIF) values for all predictor variables:\n")
#print(vif_values)
# Interpretation:
# - VIF values above 5 (or in some cases, 10) indicate multicollinearity issues.


# Calculation of predicted values for visualization
poisson_model$model$fitted <- predict(poisson_model, type = "response")
# ggplot2 Plot: Fitted vs. Actual values
ggplot(poisson_model$model) +
    geom_point(aes(x = fitted, y = AMOUNT_CLAIMS), color = "darkblue") +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") + # Diagonal line as reference
    labs(x = "Fitted Values \n", y = "Actual Number of Claims \n",
         title = "Poisson Regression: Fitted vs. Actual Number of Claims \n") +
    theme(plot.title = element_text(hjust = 0.5),
          axis.title.x = element_text(face = "bold", colour = "blue", size = 12),
          axis.title.y = element_text(face = "bold", colour = "blue", size = 12))

```
The Poisson model analysis shows no signs of overdispersion, as the overdispersion value of 0.688 is below 1. This indicates a possible underdispersion, but this is confirmed by the goodness-of-fit test (p-value = 1), which confirms a well-fitting model.

The Poisson regression model for predicting the number of claims reveals that several variables show statistically significant relationships with claim frequency. The model indicates statistically significant differences in claim frequency across categories (p-value < 0.001). The group of legal entities, which serves as the reference category, exhibits the highest claim rate. Compared to legal entities, males have a rate ratio of 0.666, reflecting a 33.4% lower claim rate, while females have the lowest claim frequency, with a rate ratio of 0.623, or 37.7% below that of legal entities.

For the insurance type (INSR_TYPE), it was found that INSR_TYPEPrivate has a rate ratio of 1.284, indicating that private insurers have a 28.4% higher claim probability compared to the reference category INSR_TYPECommercial. The variables TYPE_VEHICLE and MAKE also show significant differences in claim rates. Among vehicle types, Pick-up has the highest claim rate, with a rate ratio of 1.121, representing a 12.1% increase in claim probability compared to the reference category Automobile; however, this effect is not statistically significant (p-value = 0.120). Conversely, Motor-cycle has the lowest claim rate, with a rate ratio of 0.039, indicating an approximately 96% reduced claim probability and a highly significant result (p-value < 0.001).

Among vehicle brands, GEELY shows the highest claim rate with a rate ratio of 1.040, which, however, represents no meaningful change compared to the reference brand BISHOFTU and is statistically insignificant (p-value = 0.709). Conversely, MERCEDES has the lowest claim rate, with a rate ratio of 0.403, indicating a 59.7% lower claim probability compared to BISHOFTU and is highly significant (p-value < 0.001).

These results suggest that Pick-up and GEELY exhibit the highest, though statistically insignificant, claim rates, while Motor-cycle and MERCEDES show the lowest and statistically significant claim rates relative to their respective reference categories.

Further analysis indicates that vehicle age (AGE_VEHICLE) has a rate ratio of 0.956, meaning that the claim rate decreases by approximately 4.4% with each additional year (p-value < 0.001). The number of seats (SEATS_NUM) shows a rate ratio of 1.009, indicating that each additional seat slightly increases the claim probability, though significantly. Engine capacity (CCM_TON) shows no practical change in claim rate with a rate ratio of 1.000031, though it is statistically significant (p-value < 0.001). Insured value (INSURED_VALUE) has a rate ratio of 0.99999986, effectively showing no influence on claim frequency, although the effect is statistically significant. Premium amount (PREMIUM) exhibits a rate ratio of 1.000019, suggesting a minimal increase in claim probability with rising premiums; again, the effect is significant but very small.

The VIF values of all predictors are below the critical limit of 5, which indicates that there are no multicollinearity problems. The variables INSR_TYPE (3.75) and TYPE_VEHICLE (1.42) have the highest values, which indicates a moderate correlation with other predictors, but does not cause any stability problems in the model. Overall, the low VIF values support the robustness of the model estimates.

The plot illustrates the relationship between the predicted values (Fitted Values) and the actual number of claims (Actual Number of Claims). Most actual values are concentrated in the lower range (0 to 10), while the predicted values are almost entirely clustered near zero. The model struggles to predict higher claim counts (>10), as evident from the significant deviations for extreme values. The red dashed line, representing the ideal fit between predicted and actual values, shows that many points fall below the line, indicating a systematic underestimation of actual claims by the model. High variability or extreme values in the data can cause the model to perform poorly in capturing such cases.

The plots of estimated vs. actual values show that the Poisson model has difficulties in accurately modelling the distribution of claims, especially for higher claims values. Most of the predicted values are close to zero and systematically underestimate the actual loss frequencies as they increase. This systematic underestimation and the high number of zero claims indicate that the simple distribution of the Poisson model may not be sufficient to fully represent the structure of the data.

Given the high number of zero values in the data, a Zero-Inflated Poisson (ZIP) model could represent a useful alternative. Such a model can distinguish between structural zeros (cases where no claims occur) and random zeros (cases where claims could occur but did not), potentially improving predictive accuracy for higher claim counts without violating model assumptions about variance. As a further alternative, simplifying the model, for example by removing fewer significant variables, could be a sensible measure to improve the model.


## Binomial model

```{r Binomial_Model, cache=TRUE}

#Fitting a binomial regression model
fit.binom <- glm(CLAIM_PAID ~ SEX + INSR_TYPE +  USAGE + TYPE_VEHICLE +
                   MAKE + AGE_VEHICLE + SEATS_NUM + CCM_TON_log + INSURED_VALUE_log +
                   PREMIUM_log + AMOUNT_CLAIMS_PAID,
                 data = clean_dat_motor, 
                 family = binomial(link= "logit"))

#Inference and significance tests
#summary(fit.binom)

#Likelihood ratio test (LRT) for the significance of individual variables
#drop1(fit.binom, test = "LRT")

#Model quality: Calculation of Pseudo-R^2 values using Nagelkerke for model evaluation
nagelkerke(fit.binom)$Pseudo.R.squared.for.model.vs.null

# Calculation of the global F-test to check the significance of the entire model
TD <- fit.binom$null.deviance - fit.binom$deviance  # Test statistic
df_diff <- fit.binom$df.null - fit.binom$df.residual  # Degrees of freedom
p_value <- 1 - pchisq(TD, df = df_diff)  # p-value for the F-test
#cat("Global test statistic (TD):", TD, "\n")
#cat("p-value of the global test:", p_value, "\n")

# Checking for overdispersion in the model
deviance <- deviance(fit.binom)
df_residual <- df.residual(fit.binom)
overdispersion_ratio <- deviance / df_residual
#cat("Overdispersion Ratio (Deviance / DF):", overdispersion_ratio, "\n")

#Calculation of AIC (model quality evaluation)
aic_logit_binom <- AIC(fit.binom)
#cat("AIC of the logit model:", aic_logit_binom, "\n")

```

The analysis shows that several variables have significant associations with the likelihood of CLAIMS_PAID = Yes. According to the likelihood ratio test, the variable INSR_TYPE does not appear to have a significant impact.

Gender emerges as a key predictor: men have a 20.5% lower likelihood of receiving a claim payout compared to the reference group (legal entities), while women have a 22.4% reduced likelihood. These effects are highly significant (p < 0.001).

The vehicle type also shows significant differences. Motorcycles have approximately a 96% lower likelihood of claim payouts, making them the group with the lowest payout frequency. Similarly, station wagons and trailers exhibit significantly lower probabilities, with reductions of around 20% and 82%, respectively. In contrast, pick-ups do not show significant differences compared to the reference category (automobiles).

The vehicle brand further influences payout frequency. Vehicles from the brand DAEWOO have a 38% lower likelihood of receiving a claim payout compared to the reference brand, FIAT shows a reduction of 46%, and MERCEDES has a 51% lower likelihood. These effects are all statistically significant. Other brands, such as GEELY or MAZDA, do not show significant differences compared to the reference category.

The premium amount proves to be one of the strongest predictors. An increase in the logarithmic premium amount leads to a significant 71% increase in the likelihood of a claim payout. Conversely, a higher logarithmic insured sum reduces the payout likelihood by 25%. The claim amount also positively correlates with payout frequency: higher claim amounts significantly increase the likelihood of payouts.

The age of the vehicle has a negative effect on payout frequency: with each additional year, the likelihood of a payout decreases by approximately 2.9%. Other predictors, such as the insurance type or vehicle usage, do not have significant effects on the likelihood of a claim payout and could be excluded in a simplified model.

The pseudo-R² values (McFadden: 3.4%; Nagelkerke: 4.8%) indicate that the model explains only a small portion of the variance in the target variable, suggesting limited model performance. Nevertheless, the global test statistic (TD = 2709.954, p < 0.001) is highly significant, confirming that the model performs significantly better than a null model.

The overdispersion ratio (deviance / degrees of freedom) is 0.7766, which is less than 1, indicating that no overdispersion is present in the model. This suggests that the assumptions of the binomial model regarding data dispersion are met.

The AIC value of the logistic regression model is 77716.2.

### Method 1): Removal of (non-significant) explanatory variables

```{r Binomial_Model_sign, cache=TRUE, eval=FALSE}

# New model without INSR_TYPE
fit.binom.sign <- glm(
  CLAIM_PAID ~ SEX + USAGE + TYPE_VEHICLE + MAKE + AGE_VEHICLE + SEATS_NUM +
               CCM_TON_log + INSURED_VALUE_log + PREMIUM_log + AMOUNT_CLAIMS_PAID,
  data = clean_dat_motor,
  family = binomial(link = "logit")
)

# Display the summary of the new model (significance of individual variables)
summary(fit.binom.sign)

# Likelihood Ratio Test (LRT) for the significance of individual variables
drop1(fit.binom.sign, test = "LRT")

anova(fit.binom, fit.binom.sign, test= "LRT")

# Calculate and output the AIC of the new model
aic_no_insr_seats <- AIC(fit.binom.sign)
cat("AIC of the model without INSR_TYPE:", aic_no_insr_seats, "\n")

# Compare the AIC value with the original model
aic_difference <- aic_logit_binom - aic_no_insr_seats
cat("AIC difference compared to the original model:", aic_difference, "\n")

# Calculate the global F-test to check the significance of the new model
TD_no_insr_seats <- fit.binom.sign$null.deviance - fit.binom.sign$deviance  # Test statistic
df_diff_no_insr_seats <- fit.binom.sign$df.null - fit.binom.sign$df.residual  # Degrees of freedom
p_value_no_insr_seats <- 1 - pchisq(TD_no_insr_seats, df = df_diff_no_insr_seats)  # p-value for the F-test

cat("Global test statistic (TD) of the new model:", TD_no_insr_seats, "\n")
cat("p-value of the global test of the new model:", p_value_no_insr_seats, "\n")

# Compare the pseudo-R^2 values (model quality) using Nagelkerke
nagelkerke_new <- nagelkerke(fit.binom.sign)$Pseudo.R.squared.for.model.vs.null
print(nagelkerke_new)

# Calculate deviance residuals to check model fit
deviance_residuals <- residuals(fit.binom.sign, type = "deviance")
cat("Summary of deviance residuals:\n")
summary(deviance_residuals)

# Check for overdispersion in the model
deviance <- deviance(fit.binom.sign)
df_residual <- df.residual(fit.binom.sign)
overdispersion_ratio <- deviance / df_residual
cat("Overdispersion ratio (Deviance / DF):", overdispersion_ratio, "\n")

# Global model evaluation through likelihood ratio test against the null model
null_model <- glm(CLAIM_PAID ~ 1, data = clean_dat_motor, family = binomial(link = "logit"))
lrt_statistic <- 2 * (logLik(fit.binom.sign) - logLik(null_model))
cat("Likelihood ratio test statistic against the null model:", lrt_statistic, "\n")

# ROC curve and AUC for model evaluation
library(pROC)  # If not already loaded
predicted_probabilities <- predict(fit.binom.sign, type = "response")
roc_curve <- roc(clean_dat_motor$CLAIM_PAID, predicted_probabilities)
auc_value <- auc(roc_curve)
cat("AUC value of the model:", auc_value, "\n")

# Visualize the ROC curve
plot(roc_curve, main = "ROC Curve for Logistic Regression", col = "blue")

# Unbalanced dataset
table(clean_dat_motor$CLAIM_PAID)
```


The analysis shows that some variables, such as SEX, TYPE_VEHICLE, MAKE, AGE_VEHICLE, INSURED_VALUE_log, PREMIUM_log, and AMOUNT_CLAIMS_PAID, are highly significant (p < 0.001). These variables make a substantial contribution to explaining the variance. On the other hand, certain categories, particularly within the variables USAGE and MAKE, show low significance, indicating a limited explanatory effect of these predictors.

The model has an AIC value of 77714.2, showing only minimal improvement. However, the comparison of the null deviance (80340) with the residual deviance (77630) indicates a significant improvement over the null model. This is confirmed by the Likelihood-Ratio Test (LRT). The test comparing the model with and without the variable INSR_TYPE shows no significant difference (p = 0.945), suggesting that INSR_TYPE has no significant impact and can therefore be excluded.

The pseudo-R² values (e.g., Nagelkerke: 0.0484) indicate limited explanatory power for the model. This is supported by the AUC (Area Under the Curve) of 0.629, reflecting a low to moderate discriminatory ability. The model is only slightly better than random at distinguishing between positive and negative cases.

The analysis of deviance residuals shows a distribution close to zero, but with a maximum of 3.5154, which could indicate potential outliers. The overdispersion ratio (deviance/DF) is 0.777, suggesting no significant overdispersion in the model.

The ROC curve confirms the moderate discriminatory ability of the model. With an AUC value of 0.629, the curve demonstrates that the model predicts the target variable better than random probabilities, though it lacks strong predictive power.

In summary, the model shows some degree of significance and stability but offers only moderate explanatory power and discriminatory ability.

The imbalanced distribution of classes (CLAIMS_PAID: "YES"/"NO") in the dataset significantly impacts the model's discriminatory ability. The moderate AUC of 0.629 indicates that the model struggles to reliably recognize the minority class ("YES"). To improve discriminatory performance, balancing strategies such as oversampling, class weighting, or adjusting the decision threshold could be implemented. Additionally, exploring interaction effects might further enhance the model's performance.


## Generalised Additive Model (GAM)
A General Additive Model (GAM) is fitted to predict the number of claims based on the characteristics SEX, INSR_TYPE, USAGE, TYPE_VEHICLE, MAKE, AGE_VEHICLE, SEATS_NUM, CCM_TON, INSURED_VALUE, and PREMIUM. Similar to the Poisson model, the GAM model aims to capture the relationship between the predictors and the number of claims, allowing for non-linear relationships and interactions between variables.
```{r GAM smoothed, cache=TRUE}
# Convert unordered factors to factors
dat_amount_claims$SEX <- as.factor(dat_amount_claims$SEX)
dat_amount_claims$INSR_TYPE <- as.factor(dat_amount_claims$INSR_TYPE)
dat_amount_claims$USAGE <- as.factor(dat_amount_claims$USAGE)
dat_amount_claims$TYPE_VEHICLE <- as.factor(dat_amount_claims$TYPE_VEHICLE)
dat_amount_claims$MAKE <- as.factor(dat_amount_claims$MAKE)

# Fit a Generalized Additive Model (GAM) to the data
gam_model <- mgcv::gam(
  AMOUNT_CLAIMS ~ SEX + INSR_TYPE + USAGE + TYPE_VEHICLE + MAKE +
    s(AGE_VEHICLE, bs = "cr") +
    s(SEATS_NUM, bs = "cr") +
    s(CCM_TON, bs = "cr") +
    s(INSURED_VALUE, bs = "cr") +
    s(PREMIUM, bs = "cr"),
  data = dat_amount_claims,
  family = poisson(link = "log")
)

# Predict on the data for visualization
dat_amount_claims$predicted_claims <- predict(gam_model, 
                                              newdata = dat_amount_claims, 
                                              type = "response")

# ROC curve
library(pROC)
predicted_probabilities_gam <- predict(gam_model, type = "response")
roc_curve_gam <- roc(dat_amount_claims$AMOUNT_CLAIMS, predicted_probabilities_gam)
auc_value_gam <- auc(roc_curve_gam)
cat("AUC value of the GAM model:", auc_value_gam, "\n")

#Calculate RMSE
rmse_gam <- sqrt(mean((dat_amount_claims$AMOUNT_CLAIMS - dat_amount_claims$predicted_claims)^2))
cat("RMSE of the GAM model:", rmse_gam, "\n")

```

Different variations were used, with and without smoothing predictos and using B-splines with cubic regression. Cubic regression was chosen as it provided the best fit for the data with the smallest RMSE value, all values where ~0.02 of difference. The AUC value of the GAM model is 0.61, very similar to the results in the Poisson model. This indicates that the GAM model has a moderate ability to discriminate between the number of claims and the predictors. The RMSE value of the GAM model is 0.59, which is relatively low and indicates that the model's predictions are close to the actual values. 

```{r GAM plots}

# Visualize the GAM model
# 2x2 grid of plots, keep plots aspect ratio 1:1
par(mfrow = c(1, 2), mar = c(4, 4, 4, 2))
plot(gam_model, shade = TRUE, scale = 0)
par(mfrow = c(1, 1))
# Plot the predicted vs. actual claims
ggplot(dat_amount_claims, aes(x = predicted_claims, y = AMOUNT_CLAIMS)) +
  geom_point(color = "darkblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Predicted Claims", y = "Actual Claims",
       title = "GAM: Predicted vs. Actual Claims") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_text(face = "bold", colour = "blue", size = 12),
        axis.title.y = element_text(face = "bold", colour = "blue", size = 12))

# Plot the ROC curve
plot(roc_curve_gam, main = "ROC Curve for GAM Model", col = "blue")
```

Some interesting plots are shown, to illustrate how the predictors are related to the number of claims. The plots show the smooth functions of the predictors AGE_VEHICLE, SEATS_NUM, CCM_TON, INSURED_VALUE, and PREMIUM. The plots illustrate the non-linear relationships between these predictors and the number of claims, capturing the complex interactions and patterns in the data. 

For AGE_VEHICLE, is steady until 40 years where id declines rapidly, indicating that older vehicles have fewer claims. 

For SEATS_NUM, the number of claims increases with the number of seats related to "consumer vehicles", up to ~15 seats, where it starts to decline. There is a spike about 50 seats, probably related to commercial vehicles. The next valley and peak are related to very high number of seats, which are outliers and related to commercial or custom vehicles.

For CCM_TON, the number of claims decreases with the engine capacity untill 2500cc, which is where most of the vehicles are, from motorcicles and utility cars. After that, the number of claims increases, probably related to commercial and sport vehicles. Around 5000cc is a common engine size for commercial vehicles such as busses and trucks, after that the number of claims increases rapidly.

For INSURED_VALUE, the number of claims decreases with the insured value, indicating that more expensive vehicles have fewer claims. This is expected as more expensive vehicles are usually driven more carefully and less often.

For PREMIUM, the number of claims increases with the premium amount, starting in negative for the cheapest premium values, probably related to the insurance type and the insured value. After that, the number of claims increases with the premium amount, indicating that higher premiums are associated with more claims. This could be due to higher premiums for higher-risk drivers or vehicles or simply due to the increased value of the insured vehicles and the related need to keep it in good condition, making smaller defects a claim, which would not be claimed in cheaper vehicles.

## Neural Network
*Lead: Alvaro Cervan*


A neural network model is fitted to predict the premium amount based on the characteristics of the insured vehicles and the driver. The model is trained using the cleaned and transformed data, and the results are analyzed to evaluate the model's performance.


```{r Neural Network, results='hide'}
# Data Preparation: Split the data into training and testing sets
set.seed(123)
train_index <- sample(seq_len(nrow(clean_dat_motor)), 0.8 * nrow(clean_dat_motor))
train_data <- clean_dat_motor[train_index, ]
test_data <- clean_dat_motor[-train_index, ]

# Scale numeric features for better performance (important for nnet models)
scale_features <- function(data) {
  data$AGE_VEHICLE <- scale(data$AGE_VEHICLE)
  data$SEATS_NUM <- scale(data$SEATS_NUM)
  data$CCM_TON_log <- scale(data$CCM_TON_log)
  data$INSURED_VALUE_log <- scale(data$INSURED_VALUE_log)
  data$AMOUNT_CLAIMS_PAID <- scale(data$AMOUNT_CLAIMS_PAID)
  return(data)
}

train_data <- scale_features(train_data)
test_data <- scale_features(test_data)

# Check if the model already exists
model_path <- "nn_model.rds"
if (file.exists(model_path)) {
  nn_model <- readRDS(model_path)
  cat("Neural network model loaded from", model_path, "\n")
} else {
  # Fit the neural network model using nnet
  nn_model <- nnet(
    PREMIUM_log ~ .,
    data = train_data,
    size = 5,           # Number of neurons in the hidden layer
    linout = TRUE,      # Linear output for regression tasks
    maxit = 100,         # Max iterations for training
    trace = FALSE       # Suppress training output
  )
  
  # Save the neural network model as an RDS file
  saveRDS(nn_model, model_path)
  cat("Neural network model saved as", model_path, "\n")
}

nn_model

# visualize the neural network model
plotnet(nn_model, alpha = 0.5, margin = 0.1, cex = 0.8)
# Predict on the test data
test_data$predicted_premium_log <- predict(nn_model, test_data)

# Convert predicted log values back to the original scale
test_data$predicted_premium <- exp(test_data$predicted_premium_log)
test_data$PREMIUM <- exp(test_data$PREMIUM_log)


# Visualize Predicted vs. Actual Premium (Original Scale)
ggplot(test_data, aes(x = PREMIUM, y = predicted_premium)) +
  geom_point(color = "darkblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Actual Premium",
       y = "Predicted Premium",
       title = "Predicted vs. Actual Premium") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_text(face = "bold", colour = "blue", size = 12),
        axis.title.y = element_text(face = "bold", colour = "blue", size = 12))

# Save the neural network model as an RDS file
model_path <- "nn_model.rds"
if (!file.exists(model_path)) {
  saveRDS(nn_model, model_path)
} else {
}

# Evaluate the model
mse <- mean((test_data$PREMIUM_log - test_data$predicted_premium_log)^2)

# accuracy metrics
r_squared <- 1 - mse / var(test_data$PREMIUM_log)

rmse <- sqrt(mse)

mae <- mean(abs(test_data$PREMIUM_log - test_data$predicted_premium_log))

mape <- mean(abs(test_data$PREMIUM_log - test_data$predicted_premium_log) / test_data$PREMIUM_log) * 100

# make a table of the results
results <- data.frame(Model = "Neural Network",
                      MSE = mse,
                      R_squared = r_squared,
                      RMSE = rmse,
                      MAE = mae,
                      MAPE = mape)

#results
```

The neural network model was trained to predict the log-transformed premium amount based on the characteristics of the insured vehicles. The model was fitted using the nnet package, with the training data split into 80% training and 20% testing sets. The neural network model was trained with a hidden layer size of 5 neurons, linear output for regression tasks, and a maximum of 100 iterations for training. 

The plots of predicted vs. actual premium amounts show that the neural network model generally performs well in predicting the premium amounts. The points are clustered around the diagonal line, indicating a good alignment between the actual and predicted values. The model captures the general trend of the premiums, with some deviations for higher premium values. The model's performance can be further evaluated by considering additional metrics such as the R-squared value, RMSE, MAE, and MAPE, which provide insights into the model's accuracy and predictive power.

### Neural Network Cross Validation

Nevertheless, we cannot be sure that those values for the model above are truly correct or it was luck that the model performs well at a first instance. To solve this question, the NN will be run again using **k-fold Cross Validation** with hyperparameter tuning. This approach will help ensure that the model's performance is robust and not due to overfitting or random chance. The k-fold Cross Validation will produce a more reliable estimate of the model's performance by splitting the data into k = 10 subsets, training the model on k-1 subsets, and validating it on the remaining subset. This process is repeated k times, and the results are averaged to provide a comprehensive evaluation of the model.

```{r Neural Network Cross Validation, results='hide', cache=TRUE}
# Repeat neural network with caret and cross validation
set.seed(123)
# Define the control function for cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Define the grid of hyperparameters to tune
tune_grid <- expand.grid(size = c(3, 5, 7), decay = c(0, 0.1, 0.01))

# Check if the model already exists
model_path_cv <- "nn_model_cv.rds"
if (file.exists(model_path_cv)) {
  nn_model_cv <- readRDS(model_path_cv)
} else {
  # Train the neural network model using caret with cross-validation
  set.seed(123)
  nn_model_cv <- train(
    PREMIUM_log ~ .,
    data = train_data,
    method = "nnet",
    trControl = train_control,
    tuneGrid = tune_grid,
    linout = TRUE,
    trace = FALSE,
    maxit = 100
  )

  # Save the neural network model with cross-validation as an RDS file
  saveRDS(nn_model_cv, model_path_cv)
  cat("Neural network model with cross-validation saved as", model_path_cv, "\n")
}

nn_model_cv
plot(nn_model_cv)
# Predict on the test data using the best model
test_data$predicted_premium_log_cv <- predict(nn_model_cv, test_data)

# Convert predicted log values back to the original scale
test_data$predicted_premium_cv <- exp(test_data$predicted_premium_log_cv)

# Evaluate the model
mse_cv <- mean((test_data$PREMIUM_log - test_data$predicted_premium_log_cv)^2)

r_squared_cv <- 1 - mse_cv / var(test_data$PREMIUM_log)

rmse_cv <- sqrt(mse_cv)

mae_cv <- mean(abs(test_data$PREMIUM_log - test_data$predicted_premium_log_cv))

mape_cv <- mean(abs(test_data$PREMIUM_log - test_data$predicted_premium_log_cv) / test_data$PREMIUM_log) * 100

# print evaluations of the model as a table
results_cv <- data.frame(Model = "Neural Network Cross Validation",
                         MSE = mse_cv,
                         R_squared = r_squared_cv,
                         RMSE = rmse_cv,
                         MAE = mae_cv,
                         MAPE = mape_cv)

#results_cv

# Visualize Predicted vs. Actual Premium (Original Scale) with CV
ggplot(test_data, aes(x = PREMIUM, y = predicted_premium_cv)) +
  geom_point(color = "darkblue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Actual Premium",
       y = "Predicted Premium (CV)",
       title = "Predicted vs. Actual Premium with Cross-Validation") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title.x = element_text(face = "bold", colour = "blue", size = 12),
        axis.title.y = element_text(face = "bold", colour = "blue", size = 12))
```

## Results

| Model                          | Mean Squared Error (MSE) | R-squared | Root Mean Squared Error (RMSE) | Mean Absolute Error (MAE) | Mean Absolute Percentage Error (MAPE) |
|---------|:------:|:-----:|:----:|:------:|:----:|
| Neural Network                 | 0.2345529                | 0.7510002 | 0.4843067                     | 0.3009332                 | 3.488952 %                            |
| Neural Network Cross Validation| 0.2343864                | 0.751177  | 0.4841347                     | 0.2981682                 | 3.462526 %                            |

The weights decay plot shows how the RMSE (Root Mean Square Error) from cross-validation varies with the number of hidden units in a neural network for different weight decay values (0, 0.01, and 0.1). As the number of hidden units increases from 3 to 5, RMSE decreases across all weight decay values, suggesting improved model accuracy with additional capacity. However, beyond 5 hidden units, RMSE levels off or slightly increases, especially when weight decay is low or absent, indicating potential overfitting. Weight decay, a regularization technique to prevent overfitting, has a noticeable effect as the number of hidden units increases; while it slightly raises RMSE at lower hidden units, it helps to control error at higher hidden units. The optimal configuration, with the lowest RMSE, occurs at 5 hidden units regardless of weight decay, though weight decay of 0.1 becomes more beneficial as the model complexity increases, particularly at 6 and 7 hidden units.

The results from both the neural network and the neural network with cross-validation are very similar, with only minor differences in the evaluation metrics. This consistency suggests that the model is robust and performs well regardless of the validation method used. The cross-validation approach confirms the reliability of the neural network model, indicating that it is not overfitting and generalizes well to unseen data.

The evaluation of the neural network model revealed a Mean Squared Error (MSE) of 0.23, indicating the average squared difference between the actual and predicted log-transformed premium amounts. The R-squared value of 0.75 suggests that the model can explain approximately 75.10% of the variance in the log-transformed premiums, indicating a good fit to the data. The Root Mean Squared Error (RMSE) of 0.48 represents the square root of the MSE, providing a measure of the model's prediction accuracy. The Mean Absolute Error (MAE) of 0.30 indicates the average absolute difference between the actual and predicted log-transformed premiums. The Mean Absolute Percentage Error (MAPE) of 3.46% represents the average percentage difference between the actual and predicted premiums, providing a measure of the model's relative accuracy.

Overall, the neural network model demonstrates good performance in predicting the premium amounts based on the characteristics of the insured vehicles and drivers. The model captures the underlying patterns in the data and provides accurate predictions of the premium amounts. The evaluation metrics indicate that the model has a high level of accuracy and predictive power, which could make it a valuable tool for premium prediction in the insurance industry.


## Support Vector Machine (SVM)

*Lead: Luca Renz*

For the scenario of SVM models, it has been decided to do multiple-classifications for the premiums and divide it into 4 levels from low to very high.

```{r SVM MCC, cache=TRUE}
# Setup parallelisation
cl <- makeCluster(detectCores() - 1) 
registerDoParallel(cl)

ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

tune_grid <- expand.grid(
  .C = c(0.1, 1, 10),       
  .sigma = c(0.01, 0.1, 0.5)
)

set.seed(123)
svm_data <- clean_dat_motor[sample(nrow(clean_dat_motor), 1000, replace = FALSE), ] # only 1000 oobservations due to performance.

# Visualize the distribution and density of PREMIUM_log
hist(svm_data$PREMIUM_log, main = "Premium Distribution", xlab = "Premium", breaks = 30)
plot(density(svm_data$PREMIUM_log, na.rm = TRUE), main = "Density of Premium")

# Calculate percentiles and create custom categories for PREMIUM
percentiles <- quantile(svm_data$PREMIUM_log, probs = c(0.25, 0.5, 0.75))
svm_data$premium_category <- cut(svm_data$PREMIUM_log,
                                 breaks = c(-Inf, percentiles[1], percentiles[2], percentiles[3], Inf), 
                                 labels = c("low", "medium", "high", "very_high"))
print(table(svm_data$premium_category))

# Split into train & test data 70/30
set.seed(123)
trainIndex <- createDataPartition(svm_data$premium_category, p = 0.7, list = FALSE)
train_data <- svm_data[trainIndex, ]
test_data <- svm_data[-trainIndex, ]

# Cross validation and hypertuning
svm_model <- train(premium_category ~ SEX + AGE_VEHICLE + INSURED_VALUE_log + CLAIM_PAID_USD_log + 
                     CCM_TON_log + MAKE + USAGE,
                   data = train_data,
                   method = "svmRadial",     # radial kernel as there seems to be no linear relationship
                   trControl = ctrl,         
                   tuneGrid = tune_grid)     
stopCluster(cl)

# Evaluation of best model
print(svm_model$bestTune)     # Best hyper parameters
print(svm_model$finalModel)   # best model

print(svm_model)

# Predictions on training data
pred_train <- predict(svm_model, train_data)
conf_matrix_train <- confusionMatrix(pred_train, train_data$premium_category)
print("Confusion metrics for TEST_DATA")
print(conf_matrix_train)

mcc_train <- mccr(pred_train, train_data$premium_category)
print(paste("MCC for Train Data:", round(mcc_train, 4))) # something seems off here... manual calculation leads to roughly 0.58 whcih incidates a moderate positive correlation between predicted and actual classifications.
# manual calculation

conf_table <- as.table(conf_matrix_train)
TP <- conf_table[2, 2]  # True Positive
TN <- conf_table[1, 1]  # True Negative
FP <- conf_table[1, 2]  # False Positive
FN <- conf_table[2, 1]  # False Negative

# Calculate MCC
mcc <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))

# Print MCC
print(paste("MCC Train manually calculated:", round(mcc, 4)))

# Predictions on test data
pred_test <- predict(svm_model, newdata = test_data)

conf_matrix <- confusionMatrix(pred_test, test_data$premium_category)
print("Confusion metrics for TEST_DATA")
print(conf_matrix)

# Usage of MCC due to imbalanced dataset
mcc_test <- mccr(pred_test, test_data$premium_category)
print(paste("MCC for Test Data:", round(mcc_test, 4))) # something seems off here... manual calculation leads to roughyl 0.7 which is considered strong correlation between predicted and actual classifications.

# manual calculation

conf_table <- as.table(conf_matrix)
TP <- conf_table[2, 2]  # True Positive
TN <- conf_table[1, 1]  # True Negative
FP <- conf_table[1, 2]  # False Positive
FN <- conf_table[2, 1]  # False Negative

# Calculate MCC
mcc <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))

# Print MCC
print(paste("MCC manually calculated:", round(mcc, 4)))

```

This analysis explores the performance of a multiclass classification task using an SVM model with a radial kernel. The goal was to classify PREMIUM_log into categories: "low," "medium," "high," and "very_high," employing hyperparameter tuning and parallel computation for efficiency. Data preparation included sampling 1000 entries from the cleaned dataset and visualizing the distribution of PREMIUM_log, followed by a 70/30 split for training and testing. It is important to note that the dataset is imbalanced, which may pose challenges in model training and evaluation, potentially impacting the reliability of certain performance metrics.

Model training was carried out with 10-fold cross-validation repeated three times, leveraging parallel processing to speed up the evaluation. A grid search was performed to find optimal values for the hyperparameters c (cost) and sigma, ensuring the model's robustness and generalizability.

The evaluation showed that the "very_high" category had the highest sensitivity at 0.8629, while the "low" category excelled in specificity at 0.9695 and positive predictive value at 0.8926. The MCC for each class revealed strong performance overall: ~0.986 for "low," ~0.802 for "medium," ~0.781 for "high," and ~1.038 for "very_high," though the latter may indicate overestimation and warrants further review.

The code process incorporated parallelized cross-validation and grid search, facilitating comprehensive hyperparameter tuning. The findings highlighted an overall accuracy of 0.7771 and a Kappa statistic of 0.7029, with McNemar's test yielding a significant P-value of 0.008264 and mcc-value of roughly 0.57, suggesting a noteworthy difference from random classification.

To improve the model, checking the training set's class distribution and considering resampling techniques like random oversampling or SMOTE could be performed to further improve the model. 

In conclusion, while the model showed strong results for the "low" and "very_high" categories, further optimization is needed for "medium" and "high" to enhance overall performance.

Nevertheless, the model demonstrates reliable performance in classifying insurance premiums into the four categories with MCC of about 0.57 indicating a moderate to strong correlation between prediction and actual category.Therefore, the robust model can be used to classify premiums.Further refinement of tailored features may improve overall performance, especially for medium and high categories.


# Conclusion
#TODO
Compare models and make some final suggestions to the client where we see most potential


# Usage of Generative AI
In the group project, generative AI was employed to facilitate coding tasks, generate text, and clarify complex concepts. This technology proved beneficial for automating repetitive tasks and assisting in the assembly of report sections, especially in presenting complex ideas in a clear manner.

However, challenges were encountered in the precise formulation of prompts; imprecise prompts occasionally led to AI-generated solutions that did not meet specific project needs. Consequently, all AI-generated outputs required thorough verification to ensure their relevance and accuracy. In some instances, modifications were necessary to align the AI-produced code with project specifications or to optimize performance. The text generated by the AI also needed careful examination to confirm its alignment with project objectives and adherence to academic standards.

Generative AI struggled with tasks requiring deep contextual understanding or specialized knowledge unique to the project. While it significantly enhanced productivity and facilitated the drafting process, active human oversight was crucial to not apply irrelevant or incorrect changes.

Verification of AI suggestions against trusted sources and empirical data was consistently performed, particularly in the context of complex statistical analyses and interpretations.Using AI offered considerable advantages but required a focused and hands-on approach to fully leverage its capabilities in the academic context.
Summing up, it has definitely supported the team in terms of explaining difficult concepts while also increasing efficiency.
